\documentclass[12pt]{article}
\usepackage{Sweave,amsmath,amsfonts,bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-1ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\footnotesize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-1ex}},fontfamily=courier,fontseries=b,%
  fontsize=\footnotesize}
%%\VignetteIndexEntry{PLS vs GLS for LMMs}
%%\VignetteDepends{lme4}
\title{Penalized least squares versus generalized least squares
  representations of linear mixed models}
\author{Douglas Bates\\Department of Statistics\\%
  University of Wisconsin -- Madison}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=TRUE}
\SweaveOpts{include=FALSE}
\setkeys{Gin}{width=\textwidth}
\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\package}[1]{\textsf{\small{#1}}}
\newcommand{\trans}{\ensuremath{^\prime}}
<<preliminaries,echo=FALSE,results=hide>>=
options(width=65,digits=5)
#library(lme4)
@
\maketitle
\begin{abstract}
  The methods in the \code{lme4} package for \code{R} for fitting
  linear mixed models are based on sparse matrix methods, especially
  the Cholesky decomposition of sparse positive-semidefinite matrices,
  in a penalized least squares representation of the conditional model
  for the response given the random effects.  The representation is
  similar to that in Henderson's mixed-model equations.  An
  alternative representation of the calculations is as a generalized
  least squares problem.  We describe the two representations, show
  the equivalence of the two representations and explain why we feel
  that the penalized least squares approach is more versatile and more
  computationally efficient.
\end{abstract}
\section{Definition of the model}
\label{sec:Definition}

We consider linear mixed models in which the random effects are
represented by a $q$-dimensional random vector, $\bm{\mathcal{B}}$, and
the response is represented by an $n$-dimensional random vector,
$\bm{\mathcal{Y}}$.  We observe a value, $\bm y$, of the response.  The
random effects are unobserved.

The marginal distribution of the random effects is a multivariate
normal distribution with mean $\bm 0$ and a variance-covariance matrix,
$\bm\Sigma(\bm\theta)$, that depends on a parameter vector,
$\bm\theta$.  Typically the dimension of $\bm\theta$ is much, much
smaller than $q$.  We will defer describing a particular
parameterization until later.  For the time being we simply
characterize the marginal distribution of $\bm{\mathcal{B}}$ as
\begin{equation}
  \label{eq:remargin}
  \bm{\mathcal{B}}\sim\mathcal{N}\left(\bm 0,\bm\Sigma(\bm\theta)\right)
\end{equation}

The conditional distribution, $\bm{\mathcal{Y}}|\bm{\mathcal{B}}$, is also
multivariate normal.  The conditional mean,
$\mathrm{E}[\bm{\mathcal{Y}}|\bm{\mathcal{B}}=\bm b]$, is a linear
function of the $p$-dimensional fixed-effects parameter, $\bm\beta$,
and the $q$-dimensional random effects vector, $\bm b$, defined by the
$n\times p$ and $n\times q$ model matrices $\bm X$ and $\bm Z$ as
\begin{equation}
  \label{eq:condmean}
  \mathrm{E}[\bm{\mathcal{Y}}|\bm{\mathcal{B}}=\bm b]=
  \bm X\bm\beta+\bm Z\bm b .
\end{equation}
The conditional variance-covariance matrix of $\bm{\mathcal{Y}}$ is
simply $\sigma^2\bm I_n$, where $\bm I_n$ denotes the identity matrix
of order $n$.  Thus
\begin{equation}
  \label{eq:yconditional}
  \bm{\mathcal{Y}}|\bm{\mathcal{B}}\sim
  \mathcal{N}\left(\bm X\bm\beta+\bm Z\bm b,\sigma^2\bm I_n\right)
\end{equation}

\subsection{Variance-covariance of the random effects}
\label{sec:revarcov}

The variance-covariance matrix, $\bm\Sigma(\bm\theta)$, of the random
effects, $\bm{\mathcal{B}}$, must be symmetric and positive
semidefinite (i.e. $\bm x\trans\bm\Sigma\bm
x\ge0,\forall\bm x\in\mathbb{R}^q$).  Because the maximum likelihood
estimate of a variance component can be zero, it is important to allow
for a semidefinite $\bm\Sigma$.  That is, we do not assume that
$\bm\Sigma$ is positive definite (i.e. $\bm x\trans\bm\Sigma\bm
x>0,\forall\bm x\in\mathbb{R}^q$) and we do not assume that
$\bm\Sigma^{-1}$ exists.

A positive semidefinite matrix such as $\bm\Sigma$ has a Cholesky
decomposition of the so-called ``LDL$\trans$'' form.  We use a 
slightly modified version
\begin{equation}
  \label{eq:TSdef}
  \bm\Sigma(\bm\theta)=\sigma^2\bm T(\bm\theta)\bm S(\bm\theta)\bm
  S(\bm\theta)\bm T(\bm\theta)\trans
\end{equation}
where $\sigma$ is the same scale parameter that occurs in the
variance-covariance of $\bm{\mathcal{Y}}|\bm{\mathcal{B}}$,
$\bm T(\bm\theta)$ is a unit lower-triangular $q\times q$ matrix and
$\bm S(\bm\theta)$ is a diagonal $q\times q$ matrix with nonnegative
diagonal elements.

\subsection{Orthogonal random effects}
\label{sec:orthogonal}

Let us define a $q$-dimensional random vector, $\bm{\mathcal{U}}$, of
orthogonal random effects with a marginal distribution of
\begin{equation}
  \label{eq:Udist}
  \bm{\mathcal{U}}\sim\mathcal{N}\left(\bm 0,\sigma^2\bm I_q\right)
\end{equation}
and express $\bm{\mathcal{B}}$ as a linear transformation of
$\bm{\mathcal{U}}$, 
\begin{equation}
  \label{eq:UtoB}
  \bm{\mathcal{B}}=\bm T\bm S\bm{\mathcal{U}} .
\end{equation}
Note that the transformation (\ref{eq:UtoB}) gives the desired
distribution of $\bm{\mathcal{B}}$ in that
$\mathrm{E}[\bm{\mathcal{B}}]=\bm T\bm
S\mathrm{E}[\bm{\mathcal{U}}]=\bm 0$ and
\begin{displaymath}
  \mathrm{Var}(\bm{\mathcal{B}})=\mathrm{E}[\bm{\mathcal{B}}\bm{\mathcal{B}}\trans]
  =\bm T\bm S\mathrm{E}[\bm{\mathcal{U}}\bm{\mathcal{U}}\trans]\bm
  S\bm T\trans=\sigma^2\bm T\bm S\bm S\bm T\trans=\bm\Sigma .
\end{displaymath}

The conditional distribution, $\bm{\mathcal{Y}}|\bm{\mathcal{U}}$, can
be derived from $\bm{\mathcal{Y}}|\bm{\mathcal{B}}$ as
\begin{equation}
  \label{eq:YgivenU}
  \bm{\mathcal{Y}}|\bm{\mathcal{U}}\sim\mathcal{N}\left(\bm X\bm\beta+\bm
  Z\bm T\bm S\bm u, \sigma^2\bm I\right)
\end{equation}
We will write the transpose of $\bm Z\bm T\bm S$ as $\bm A$.  Because
the matrices $\bm T$ and $\bm S$ depend on the parameter $\bm\theta$,
$\bm A(\bm\theta)$ is also a function of $\bm\theta$.  That is
\begin{equation}
  \label{eq:Adef}
  \bm A\trans(\bm\theta)=\bm Z\bm T(\bm\theta)\bm S(\bm\theta) .
\end{equation}

\subsection{Sparse matrix methods}
\label{sec:sparseMatrix}

The reason for the peculiar definition of $\bm A$ as the transpose of
the model matrix is because $\bm A$ is stored and manipulated as a
sparse matrix.  In the compressed column-oriented storage form for
sparse matrices there are advantages to storing $\bm A$ as a matrix of
$n$ columns and $q$ rows.  In particular, the CHOLMOD sparse matrix
library allows us to evaluate the sparse
Cholesky factor, $\bm L$, a lower triangular matrix that satisfies
\begin{equation}
  \label{eq:SparseChol}
  \bm L(\bm\theta)\bm L(\bm\theta)\trans=
  \bm P\left(\bm A(\bm\theta)\bm A(\bm\theta)\trans+\bm I_q\right)\bm P\trans ,
\end{equation}
directly from $\bm A$.  

In (\ref{eq:SparseChol}) the $q\times q$
matrix $\bm P$ is a ``fill-reducing'' permutation matrix determined
from the pattern of nonzeros in the sparse model matrix $\bm Z$.  It
does not affect the statistical theory (if
$\bm{\mathcal{U}}\sim\mathcal{N}(\bm 0,\sigma^2\bm I)$ then $\bm
P\trans\bm{\mathcal{U}}$ also has a $\mathcal{N}(\bm 0,\sigma^2\bm I)$
distribution because $\bm P\bm P\trans=\bm P\trans\bm P=\bm I$) but,
because it affects the number of nonzeros in $\bm L$, it
can have a tremendous impact on the amount storage required for $\bm
L$ and the time required to evaluate it.  Indeed, it is precisely
because $\bm L(\bm\theta)$ can be evaluated quickly, even for complex
models applied the large data sets, that the \code{lmer} function is
effective in fitting such models.

\section{The penalized least squares approach to linear mixed models}
\label{sec:Penalized}

Given a value of $\bm\theta$ we form $\bm A(\bm\theta)$ from which we
evaluate $\bm L(\bm\theta)$.  We can then solve for the $q\times p$
matrix, $\bm R_{\bm{ZX}}$, in the system of equations
\begin{equation}
  \label{eq:RZX}
  \bm L(\theta)\bm R_{\bm{ZX}}=\bm P\bm A(\bm\theta)\bm X
\end{equation}
and for the $p\times p$ upper triangular matrix, $\bm R_{\bm X}$, satisfying
\begin{equation}
  \label{eq:RX}
  \bm R_{\bm X}\trans\bm R_{\bm X}=
  \bm X\trans\bm X-\bm R_{\bm{ZX}}\trans\bm R_{\bm{ZX}}
\end{equation}

The conditional mode, $\tilde{\bm u}(\bm\theta)$, of the
orthogonal random effects and the conditional mle,
$\widehat{\bm\beta}(\bm\theta)$, of the fixed-effects parameters
can be determined simultaneously as the solutions to a penalized least
squares problem
\begin{equation}
  \label{eq:PLS}
  \begin{bmatrix}
    \tilde{\bm u}(\bm\theta)\\
    \widehat{\bm\beta}(\bm\theta)
  \end{bmatrix}=
  \arg\min_{\bm u,\bm\beta}\left\|
    \begin{bmatrix}\bm y\\\bm 0\end{bmatrix} -
    \begin{bmatrix}
      \bm A\trans\bm P\trans & \bm X\\
      \bm I_q & \bm 0
    \end{bmatrix}
    \begin{bmatrix}\bm u\\\bm\beta\end{bmatrix}
  \right\|^2
\end{equation}
for which the solution satisfies
\begin{equation}
  \label{eq:PLSsol}
  \begin{bmatrix}
    \bm P\left(\bm A\bm A\trans+\bm I\right)\bm P\trans &
    \bm P\bm A\bm X\\
    \bm X\trans\bm A\trans\bm P\trans & \bm X\trans\bm X
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\bm u}(\bm\theta)\\
    \widehat{\bm\beta}(\bm\theta)
  \end{bmatrix}=
  \begin{bmatrix}\bm P\bm A\bm y\\\bm X\trans\bm y\end{bmatrix}
\end{equation}
The Cholesky factor of the system matrix for the PLS problem can be
expressed using $\bm L$, $\bm R_{\bm Z\bm X}$ and $\bm R_{\bm X}$ because
\begin{equation}
  \label{eq:PLSChol}
  \begin{bmatrix}
    \bm P\left(\bm A\bm A\trans+\bm I\right)\bm P\trans & \bm P\bm
    A\bm X\\
    \bm X\trans\bm A\trans\bm P\trans & \bm X\trans\bm X
  \end{bmatrix} =
  \begin{bmatrix}
    \bm L & \bm 0\\
    \bm R_{\bm Z\bm X}\trans & \bm R_{\bm X}\trans
  \end{bmatrix}
  \begin{bmatrix}
    \bm L\trans & \bm R_{\bm Z\bm X}\\
    \bm 0 & \bm R_{\bm X}
  \end{bmatrix} .
\end{equation}

In the \code{lme4} package the \code{"mer"} class is the
representation of a mixed-effects model.  The \code{A} slot contains
the sparse matrix $\bm A(\bm\theta)$ and the \code{L} slot contains
the Cholesky factor $\bm L(\bm\theta)$ satisfying
(\ref{eq:SparseChol}).  The \code{RZX} and \code{RX} slots contain
$\bm R_{\bm Z\bm X}(\bm\theta)$ and $\bm R_{\bm X}(\bm\theta)$
as dense matrices.

It is not necessary to solve for $\tilde{\bm u}(\bm\theta)$ and
$\widehat{\bm\beta}(\bm\theta)$ to evaluate the profiled
log-likelihood as a function of $\bm\theta$, which is the
log-likelihood evaluated at $\bm\theta$,
$\widehat{\bm\beta}(\bm\theta)$ and $\widehat{\sigma^2}(\bm\theta)$.
All that is needed for evaluation of
the profiled log-likelihood is the penalized residual sum of squares,
$r^2$, and the determinant, $|\bm A\bm A\trans+\bm I|=|\bm L|^2$. 

Because $\bm L$ is triangular, its determinant is simply the product
of its diagonal elements and, because $\bm A\bm A\trans + \bm I$ is
positive definite, $|\bm L|^2>0$.

The profiled deviance (negative twice the profiled log-likelihood), as
a function of $\bm\theta$ only ($\bm\beta$ and $\sigma^2$ at their
conditional estimates), is
\begin{equation}
  \label{eq:profiledDev}
  d(\bm\theta|\bm y)=\log(|\bm L|^2)+n\left(1+\log(r^2)+\frac{2\pi}{n}\right)
\end{equation}
The maximum likelihood estimates, $\widehat{\bm\theta}$, satisfy
\begin{equation}
  \label{eq:thetamle}
  \widehat{\bm\theta}=\arg\min_{\bm\theta}d(\bm\theta|\bm y)
\end{equation}
Once the value of $\widehat{\bm\theta}$ has been determined, the mle's
of the other parameters are evaluated from (\ref{eq:PLSsol}) and
\begin{equation}
  \label{eq:sigmahat}
  \widehat{\sigma^2}(\bm\theta)=\frac{r^2}{n} .
\end{equation}

\subsection{Comments on the sparse matrix representation}
\label{sec:commentsSparse}

Note that nothing has been said about the form of the sparse model
matrix $\bm Z$ other than the fact that it is sparse.  The
computational methods outlined above can be applied to models with
multiple random effects terms in which the factors determining the
random effects are nested or crossed or partially crossed.


\section{The generalized least squares approach to linear mixed models}
\label{sec:GLS}

Another common approach to linear mixed models is to derive the
marginal variance-covariance matrix of $\bm{\mathcal{Y}}$ as a
function of $\bm\theta$ and use that to determine the conditional
estimates, $\widehat{\bm\beta}(\bm\theta)$, as the solution of a
generalized least squares (GLS) problem.  In the notation of
\S\ref{sec:Definition} the marginal mean of $\bm{\mathcal{Y}}$ is
$\mathrm{E}[\bm{\mathcal{Y}}]=\bm X\bm\beta$ and the marginal
variance-covariance matrix is
\begin{equation}
  \label{eq:marginalvarcovY}
  \mathrm{Var}(\bm{\mathcal{Y}})=\sigma^2\left(\bm I_n+\bm Z\bm T\bm
    S\bm S\bm T\trans\bm Z\trans\right)=\sigma^2\left(\bm I_n+\bm
    A\trans\bm A\right) =\sigma^2\bm V(\bm\theta) ,
\end{equation}
where $\bm V(\bm\theta)=\bm I_n+\bm A\trans\bm A$.

The conditional estimates of $\bm\beta$ are often written as
\begin{equation}
  \label{eq:condbeta}
  \widehat{\bm\beta}(\bm\theta)=\left(\bm X\trans\bm V^{-1}\bm
    X\right)^{-1}\bm X\trans\bm V^{-1}\bm y
\end{equation}
but, of course, this formula is not suitable for computation.  The
matrix $\bm V(\bm\theta)$ is a symmetric $n\times n$ positive definite
matrix and hence has a Cholesky factor.  However, this factor is
$n\times n$, not $q\times q$ and $q$ is always smaller than $n$ -
sometimes orders of magnitude smaller.

\subsection{Relating the GLS approach to the Cholesky factor $L$.}
\label{sec:GLStoL}

We can use the fact that
\begin{equation}
  \label{eq:Vinv}
  \bm V^{-1}(\bm\theta)=\left(\bm I_n+\bm A\trans\bm A\right)^{-1}=
  \bm I_n-\bm A\trans\left(\bm I_q+\bm A\bm A\trans\right)^{-1}\bm A
\end{equation}
to relate the GLS problem to the PLS problem.  One way to establish
(\ref{eq:Vinv}) is simply to show that the product
\begin{multline*}
  (\bm I+\bm A\trans\bm A)\left(\bm I-\bm A\trans\left(\bm I+\bm A\bm
      A\trans\right)^{-1}\bm A\right)\\
  \begin{aligned}
    =&\bm I+\bm A\trans\bm A-\bm A\trans\left(\bm I+\bm A\bm
      A\trans\right)
    \left(\bm I+\bm A\bm A\trans\right)^{-1}\bm A\\
    =&\bm I+\bm A\trans\bm A-\bm A\trans\bm A\\
    =&\bm I .
  \end{aligned}
\end{multline*}
Incorporating the permutation matrix $\bm P$ we have
\begin{equation}
  \label{eq:PLA}
  \begin{aligned}
    \bm V^{-1}(\bm\theta)=&\bm I_n-\bm A\trans\bm P\trans\bm P\left(\bm
      I_q+\bm A\bm A\trans\right)^{-1}\bm P\trans\bm P\bm A\\
    =&\bm I_n-\bm A\trans\bm P\trans(\bm L\bm L\trans)^{-1}\bm P\bm A\\
    =&\bm I_n-\left(\bm L^{-1}\bm P\bm A\right)\trans\bm L^{-1}\bm P\bm A .
  \end{aligned}
\end{equation}
Even in this form we would not want to evaluate such a matrix but
(\ref{eq:PLA}) does allow us to simplify many common expressions.

For example, the variance-covariance of the estimator $\widehat{\bm
  \beta}$, conditional on $\bm\theta$ and $\sigma$, can be expressed as
\begin{equation}
  \label{eq:varcovbeta}
  \begin{aligned}
  \sigma^2\left(\bm X\trans\bm V^{-1}(\bm\theta)\bm X\right)^{-1}
  =&\sigma^2\left(\bm X\trans\bm X-\left(\bm L^{-1}\bm P\bm
      A\bm X\right)\trans\left(\bm L^{-1}\bm P\bm A\bm
      X\right)\right)^{-1}\\
  =&\sigma^2\left(\bm X\trans\bm X-\bm R_{\bm Z\bm X}\trans\bm
    R_{\bm Z\bm X}\right)^{-1}\\
    =&\sigma^2\left(\bm R_{\bm X}\trans\bm R_{\bm X}\right)^{-1} .
  \end{aligned}
\end{equation}
\end{document}
