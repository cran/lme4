\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{myVignette}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{Implementation Details}
%%\VignetteDepends{Matrix}
%%\VignetteDepends{lattice}
%%\VignetteDepends{lme4}
\begin{document}


\setkeys{Gin}{width=\textwidth}
\title{Linear mixed model implementation in lme4}
\author{Douglas Bates\\Department of Statistics\\University of
  Wisconsin -- Madison\\\email{Bates@wisc.edu}}
\maketitle
\begin{abstract}
  Expressions for the evaluation of the profiled log-likelihood or
  profiled log-restricted-likelihood of a linear mixed model, the
  gradients and Hessians of these criteria, and update steps for an
  ECME algorithm to optimize these criteria are given in Bates and
  DebRoy (2004). In this paper we generalize those formulae and
  describe the representation of mixed-effects models using sparse
  matrix methods available in the \code{Matrix} package.
\end{abstract}

\section{Introduction}
\label{sec:Intro}

General formulae for the evaluation of the profiled log-likelihood and
profiled log-restricted-likelihood in a linear mixed model are given
in \citet{bate:debr:2004}.  Here we describe a more general
formulation of the model using sparse matrix decompositions and
describe the implementation of these methods in the \code{lmer}
function for \RR{}.

In \S\ref{sec:Form} we describe the form and representation of the
model.  The calculation of the criteria to be optimized by the
parameter estimates and related quantities is discussed in
\S\ref{sec:likelihood}.

% Details of the calculation of the ECME step and the
% evaluation of the gradients of the criteria are given in
% \S\ref{sec:ECME} and those of the Hessian in \S\ref{sec:Hessian}.  In
% \S\ref{sec:Unconstrained} we give the details of an unconstrained
% parameterization for the model and the transformation of our
% results to this parameterization.

\section{Form and representation of the model}
\label{sec:Form}

We consider linear mixed models of the form
\begin{equation}
  \label{eq:lmeGeneral}
  \by=\bX\bbeta+\bZ\bb+\beps\quad
  \beps\sim\mathcal{N}(\bzer,\sigma^2\bI),
  \bb\sim\mathcal{N}(\bzer,\bSigma),
  \beps\perp\bb
\end{equation}
where $\by$ is the $n$-dimensional response vector, $\bX$ is an
$n\times p$ model matrix for the $p$ dimensional fixed-effects vector
$\bbeta$, $\bZ$ is the $n\times q$ model matrix for the $q$
dimensional random-effects vector $\bb$, which has a Gaussian
distribution with mean $\bzer$ and variance-covariance matrix
$\bSigma$, and $\beps$ is the random noise assumed to have a spherical
Gaussian distribution.  The symbol $\perp$ indicates independence of
random variables.

We will assume that $\bX$ has full column rank and that $\bSigma$ is
positive definite.

\subsection{Structure of the variance-covariance matrix}
\label{sec:sigmaStructure}

Components of the random effects vector $\bb$ and portions of its
variance-covariance matrix $\bSigma$ are associated with $k$ grouping
factors $\bbf_i, i=1,\dots,k$, each of length $n$, and with the $n_i,
i = 1,\dots,k$ levels of each of the grouping factors.  In general
there are $q_i$ components of $\bb$ associated with each of the $n_i$
levels the grouping factor $\bbf_i, i = 1,\dots,k$.  Thus
\begin{equation}
  \label{eq:qdef}
  q = \sum_{i=1}^k n_i q_i 
\end{equation}

We assume that the components of $\bb$ and the rows and columns of
$\bSigma$ are ordered according to the $k$ grouping factors and,
within the block for the $i$th grouping factor, according to the $n_i$
levels of the grouping factor.

Random effects associated with different grouping factors are
independent.  This implies that $\bSigma$ is block-diagonal with $k$
diagonal blocks of orders $n_i q_i, i=1,\dots,k$.

Random effects associated with different levels of the same grouping
factor are independent.  This implies that the $i$th (outer) diagonal
block of $\bSigma$ is itself block diagonal in $n_i$ blocks of order $q_i$.
We say that the structure of $\bSigma$ is block/block diagonal.

Finally, the variance-covariance matrix within each of the
$q_i$-dimensional subvectors of $\bb$ associated with one of the $n_i$
levels of grouping factor $\bbf_i, i=1,\dots,k$ is a constant (but
unknown) positive-definite symmetric $q_i\times q_i$ matrix
$\bSigma_i,i=1,\dots,k$.  This implies that each of the $n_i$ inner
diagonal blocks of order $q_i$ is a copy of $\bSigma_i$.  We say that
$\bSigma$ has a \emph{repeated block/block diagonal} structure.

In the notation of the Kronecker product, the $i$th outer diagonal
block of $\bSigma$ is $\bI_{n_i}\otimes\bSigma_i$.

\subsection{The relative precision matrix}
\label{sec:relativePrecision}

Many of the computational formulae that follow are more conveniently
expressed in terms of $\bSigma^{-1}$, which is called the
\emph{precision} matrix of the random effects, than in terms of
$\bSigma$, the variance-covariance matrix.  In fact, the formulae are
most conveniently expressed in terms of the \emph{relative precision
  matrix} $\sigma^2\bSigma^{-1}$ which we write as $\bOmega$.  That
is,
\begin{equation}
  \label{eq:relPrec}
  \bOmega = \sigma^2\bSigma^{-1}
\end{equation}

This called the ``relative'' precision because it is precision of
$\bb$ (i.e.{} $\bSigma^{-1}$) relative to the precision of $\beps$
(i.e.{} $\sigma^{-2}\bI$).

It is easy to establish that $\bOmega$ will have a repeated
block/block diagonal structure like that of $\bSigma$.  That is,
$\bOmega$ consists of $k$ outer diagonal blocks of sizes $n_i q_i, i =
1,\dots,k$ and the $i$th outer diagonal block is itself block diagonal
with $n_i$ inner blocks of size $q_i\times q_i$.  Furthermore, each of
the inner diagonal blocks in the $i$th outer block is a copy of the
$q_i\times q_i$ positive-definite, symmetric matrix $\bOmega_i$.

Because $\bOmega$ has a repeated block/block structure we can define
the entire matrix by specifying the symmetric matrices $\bOmega_i, i =
1,\dots,k$ and, because of the symmetry, $\bOmega_i$ has at most
$q_i(q_i+1)/2$ distinct elements.  We will write a parameter vector of
length at most $\sum_{i=1}^{k}q_i(q_i+1)/2$ that determines $\bOmega$
as $\btheta$.  For example, we could define $\btheta$ to be the
non-redundant elements in the $\bOmega_i$, although in the actual
computations we use a different, but equivalent, parameterization for
reasons to be discussed later.

We only need to store the matrices $\bOmega_i,i=1,\dots,k$ and the
number of levels in the grouping factors to be able to create $\bOmega$.
The matrices $\bOmega_i$ are stored in the \code{Omega} slot of an
object of class \code{"lmer"}. The values of $k$ and $n_i,
i=1,\dots,k$ can be determined from the list of the grouping factors
themselves, stored as the \code{flist} slot, or from the dimensions
$q_i,i=1,\dots,k$, stored in the \code{nc} slot, and the group
pointers, stored in the \code{Gp} slot.  The group pointers are the
(0-based) indices of the first component of $\bb$ associated with the
$i$th grouping factor.  The last element of \code{Gp} is the number of
elements in $\bb$.

Thus successive differences of the group pointers are the total number
of components of $\bb$ associated with the $i$th grouping factor.
That is, these differences are $n_i q_i,i=1,\dots,k$.  The first
element of the \code{Gp} slot is always 0.

\subsection{Examples}
\label{sec:OmegaExamp}

Consider the fitted models
\begin{Schunk}
\begin{Sinput}
> Sm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)