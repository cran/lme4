\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{myVignette}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{Sparse matrix representations of linear mixed models}
%%\VignetteDepends{Matrix}
%%\VignetteDepends{lme4}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=5,height=3,strip.white=TRUE}
\setkeys{Gin}{width=\textwidth}
\title{Sparse Matrix Representations of Linear Mixed Models}
\author{Douglas Bates\\R Development Core Team\\\email{Douglas.Bates@R-project.org}}
\date{\today}
\maketitle
\begin{abstract}
  We describe a representation of linear mixed-effects models using
  positive semidefinite, symmetric, compressed, column-oriented,
  sparse matrices.  This representation provides for efficient
  evaluation of the profiled log-likelihood or profiled restricted
  log-likelihood of the model, given the relative precision parameters
  of the random effects.  The evaluation is based upon the
  $\bL\bD\bL\trans$ form of the Cholesky decomposition of the
  augmented sparse representation.  Additionally, we can use
  information from this representation to evaluate ECME updates and
  the gradient of the criterion being optimized.
  
  The sparse matrix methods that we employ have both a symbolic phase,
  in which the number and positions of the off-diagonal elements
  are determined, and a numeric phase, in which the actual numeric
  values are determined.  The symbolic phase need only be done once
  and it can be accomplished knowing only the grouping factors with
  which the random effects are associated. An important part of the
  symbolic phase is determination of a fill-minimizing permutation of
  the rows and columns of a sparse semi-definite matrix.  This matrix
  have a special structure in the linear mixed-effects problem and we
  provide a new fill-minimizing algorithm tuned to this structure.

\end{abstract}
<<preliminaries,echo=FALSE,print=FALSE>>=
options(width=75)
library(lme4)
data(ScotsSec, package = "Matrix")
@

\section{Introduction}
\label{sec:Intro}

Mixed-effects models, also called multilevel models, panel data
models, and frailty models, are widely used in many areas of
statistical applications \citep{pinh:bate:2000}.  The basic form of
the model, the linear mixed model, also serves as an approximation in
iterative estimation of the parameters in more general forms such as
the generalized linear mixed model (GLMM) and the nonlinear mixed
model (NMM).

In \S\ref{sec:LinearMixed} we define a general form of a linear mixed
model using grouping factors and model matrices that are associated
with the grouping factors.  This form, which can be used for multiple
levels of random effects in either nested or crossed configurations,
can be represented and manipulated using a sparse, symmetric,
semidefinite matrix and several dense matrices.  We show that a
profiled log-likelihood can be evaluated from that solution of a
penalized least squares problem and that this solution can be obtained
from the Cholesky decomposition of an augmented form of the sparse,
symmetric matrix.

Many implementations of the Cholesky decomposition of sparse,
symmetric, semidefinite matrices have both a symbolic phase, in which
the number and positions of the off-diagonal elements are determined,
and a numeric phase, in which the actual numeric values are
determined.  In \S\ref{sec:Symbolic} we show that the symbolic
analysis for the matrices we consider need only be done once and can
be accomplished knowing only the grouping factors.  An important part
of the symbolic phase is determination of a fill-reducing permutation
of the rows and columns of the symmetric matrix.  We show that by
suitably ordering the grouping factors and by restricting ourselves to
permutations that correspond to reorderings of the levels within the
grouping factors we can determine effective fill-reducing orderings.

Finally, in \S\ref{sec:Extensions} we show how these methods can be
used to implement general penalized least squares approaches to models
such as the GLMM and the NMM and then to implement more accurate
approximations to the marginal likelihood using Laplacian integration
or adaptive Gauss-Hermite integration.

\section{Linear mixed models}
\label{sec:LinearMixed}

We describe the form of the linear mixed-effects model that we
consider and restate some of the formulas from \citet{bate:debr:2004}
using the LDL form of the Cholesky decomposition of a sparse,
semi-definite matrix.

\subsection{Form of the model}
\label{sec:ModelForm}

We consider linear mixed-effects models that can be written as
\begin{equation}
  \label{eq:lmeGeneral}
  \by=\bX\bbeta+\bZ\bb+\beps\quad
  \beps\sim\mathcal{N}(\bzer,\sigma^2\bI),
  \bb\sim\mathcal{N}(\bzer,\sigma^2\bOmega^{-1}),
  \beps\perp\bb
\end{equation}
where $\by$ is the $n$-dimensional response vector, $\bX$ is an
$n\times p$ model matrix for the $p$-dimensional fixed-effects vector
$\bbeta$, $\bZ$ is the $n\times q$ model matrix for the
$q$-dimensional random-effects vector $\bb$ that has a Gaussian
distribution with mean $\bzer$ and relative precision matrix $\bOmega$
(i.e., $\bOmega$ is the precision of $\bb$ relative to the precision
of $\beps$), and $\beps$ is the random noise assumed to have a
spherical Gaussian distribution.  The symbol $\perp$ indicates
independence of random variables.  We assume that $\bX$ has full
column rank and that $\bOmega$, which is a function of an
(unconstrained) parameter vector $\btheta$, is positive definite.


\subsubsection{Grouping factors for the random effects}

Although $q$, the dimension of the vector $\bb$ (and, correspondingly,
the number of columns in $\bZ$ and the number of rows and columns in
$\bZ\trans\bZ$ and $\bOmega$) can be very large, these vectors and
matrices can be divided into components associated with grouping
factors $\bff_i,i=1,\dots,k$ in the data.  Each grouping factor is of
length $n$, the same as the length of $\by$.  The number of distinct
values in $\bff_i$, also called the number of \emph{levels} of
$\bff_i$, is $m_i,i=1,\dots,k$.  In the general form of the model, a
model matrix $\bZ_i$ of size $n\times q_i$ is associated with grouping
factor $\bff_i$, $i=1,\dots,k$.  Typically the $q_i$ are very small.
In fact, in one common form of the model, called a \emph{variance
components} model, $q_1=q_2=\dots=q_k=1$ and each of the
$\bZ_i,i=1,\dots,k$ consist of a single column of 1's.

In the general form, the random effects vector $\bb$, of length
$q=\sum_{i=1}^k m_i q_i$, is partitioned into $k$ ``outer blocks''
where the i'th outer block is of size $m_i q_i,i=1,\dots,k$.  The
columns of $\bZ$ and the rows and columns of $\bZ\trans\bZ$ and
$\bOmega$ are similarly partitioned.  Each of the outer blocks is
further subdivided into $m_i$ inner blocks of size $q_i$.  The
grouping factors determine the outer blocks and the levels of each
grouping factor determine the inner blocks.

In the models that we will consider, the random effects associated
with different grouping factors are independent.  That is,
$\bOmega$ is block-diagonal in $k$ blocks of sizes
$m_i q_i\times m_i q_i,i=1,\dots,k$.  Furthermore, the random effects
associated with the levels of a given blocking factor are independent
and identically distributed.  Thus the $i$'th diagonal block in
$\bOmega$ is itself block diagonal and these diagonal blocks are $m_i$
repetitions of a $q_i\times q_i$ matrix $\bOmega_i$, $i=1,\dots,k$, providing
\begin{equation}
  \label{eq:OmegaDet}
  |\bOmega|=\sum_{i=1}^k m_i |\bOmega_i|
\end{equation}

For a variance components model the matrices $\bOmega_i,i=1,\dots,k$
are $1\times 1$ positive definite matrices which we can consider to be
positive scalars $\omega_i,i=1,\dots,k$.  The matrix $\bOmega$ is
block-diagonal of size $\sum_{i=1}^k m_i$ and the diagonal blocks are
$\omega_i\bI_{m_i}$ where $\bI_{m_i}$ is the $m_i\times m_i$ identity
matrix.  Thus $\left|\bOmega\right|=\sum_{i=1}^k m_i\omega_i$. The
$k$-dimensional vector $\btheta$ where
$\theta_i=\log\omega_i,i=1,\dots,k$ can be used as the unconstrained
parameter vector.

The columns of the matrix $\bZ$ are similarly divided into blocks.
For the variance components model the $i$th block is the set of
indicator columns for the $m_i$ levels of $\bff_i,i=1,\dots,k$.
Because each block is a set of indicators, the diagonal blocks of
$\bZ\trans\bZ$ are themselves diagonal.  However, unlike the
corresponding blocks in $\bOmega$, these blocks are not necessarily a
multiple of the identity.  The diagonal elements of the $i$th diagonal
block are the $m_i$ frequencies of occurence of each the levels of the
$i$th grouping factor in the data.  (Because all the elements of $\bZ$
are zero or one, the diagonals of $\bZ\trans\bZ$ are simply the counts
of the number of ones in the corresponding column of $\bZ$.)

The off-diagonal blocks of $\bZ\trans\bZ$ in a variance components
model are the pairwise cross-tabulations of the corresponding grouping
factors.


\subsubsection{The Scottish secondary school example}
\label{sec:Scottish}

An example may help to clarify these descriptions.

Data on achievement scores of Scottish secondary school students are
described in \citet{Paterson:1991} and are analyzed in
\citet[ch.~18]{MLwiN:2002} and other references.  In the \code{Matrix}
package for \RR{} these data are available as the data set
\var{ScotsSec} containing the achievement scores (\var{attain}), some
demographic data (\var{sex} and \var{social} class), a \var{verbal}
reasoning score based on tests taken at entry to secondary school, and
the \var{primary} and secondary (\var{second}) schools attended by
3435 students.

The grouping factors for the random effects are \var{primary} (148
distinct schools) and \var{second} (19 distinct schools).  
the locations of the non-zeros in the $167\times 167$ matrix
$\bZ\trans\bZ$ are shown in Figure~\ref{fig:ZtZ}.
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<FigZtZ,fig=TRUE,echo=FALSE,height=8,width=8.>>=
ctab = sscCrosstab(list(ScotsSec$primary, ScotsSec$second), FALSE)
print(image(ctab, aspect='xy', xlim = c(-0.5,166.5),
           ylim = c(166.5, -0.5), colorkey = FALSE))
@   
\caption{Location of non-zero elements in
  $\bZ\trans\bZ$ for a variance components model for the
  \var{ScotsSec} data.  Darker squares indicate larger magnitudes.
  Rows and columns are numbered from zero.  The first 148 rows and
  columns correspond to the levels of the \var{primary} grouping
  factor and the last 19 rows and columns correspond to levels of the
  \var{second} grouping factor.}
  \label{fig:ZtZ}
\end{figure}
for a
variance components model with these grouping factors.
In the figure darker greys indicate larger magnitudes.


\subsubsection{General structure of the sparse matrix}
\label{sec:Sparse}

For the variance components model $bZ\trans\bZ$ is based on the
pairwise cross-tabulation of the grouping factors.  In the more
general model where some of the $\bZ_i$ can have multiple columns, the
structure of $\bZ\trans\bZ$ can be derived from the structure of the
pairwise cross-tabulation matrix.  Both $\bZ\trans\bZ$ and the
pairwise cross-tabulation can be divided into a $k\times k$ grid of
blocks.  The pattern of non-zeros in the $(i,j)$ block of
$\bZ\trans\bZ$ is obtained by replacing each non-zero in the $(i,j)$
block of the cross-tabulation by a $q_i\times q_j$ matrix.  Notice
that we can determine the patterns of non-zeros in $\bZ\trans\bZ$
knowing only the $q_i,i=1,\dots,k$ and the cross-tabulation of the
grouping factors.

\subsubsection{Crossed and nested grouping factors}
\label{ssec:Crossed}

In the Scottish secondary school example if all the students from a
given primary school attended the same secondary school we would say
that \var{primary} is \emph{nested within} \var{second}.  That is not
the case.  We can see in Figure~\ref{fig:ZtZ} that there is a moderate
amount of \emph{crossing} of these two grouping factors.  If there was
at least one student in the study from each combination of primary
school and secondary school we would describe the grouping factors
\var{primary} and \var{second} as being \emph{fully crossed}.  Again,
that is not the case for the Scottish secondary data.  Grouping
factors like these, which are neither nested nor fully crossed, are
said to be \emph{partially crossed}.

\subsection{Estimation criteria and related quantities}
\label{sec:ModelCriteria}

For ease of reference we restate some of the results from
\citet{bate:debr:2004} in the form in which they will be calculated.

Given the observed responses $\by$ and the model matrices $\bX$ and
$\bZ$, we wish to determine either the maximum likelihood (ML) or the
restricted maximum likelihood (REML) estimates of the parameters
$\btheta$, $\bbeta$, and $\sigma^2$.  Because the conditional
estimates of $\bbeta$ and $\sigma^2$, given a value of $\btheta$, for
either criterion can be determined from the solution to penalized
least squares problem, we can reduce the optimization problem to one
involving $\btheta$ only.  This reduction of the dimension of the
optimization problem is called \emph{profiling} the objective.

The conditional, penalized least squares problem can be solved using
the Cholesky decomposition
\begin{equation}
  \label{eq:CrossProdGen}
  \begin{bmatrix}
    \bZ\trans\bZ+\bOmega & \bZ\trans\bX  & \bZ\trans\by \\
    \bX\trans\bZ         & \bX\trans\bX  & \bX\trans\by \\
    \by\trans\bZ         & \by\trans\bX  & \by\trans\by
  \end{bmatrix}=\bR\trans\bR\quad\text{where}\quad\bR=
  \begin{bmatrix}
    \RZZ & \RZX & \rZy \\
    \bzer    & \RXX & \rXy \\
    \bzer    & \bzer    & \ryy
  \end{bmatrix} .
\end{equation}
where the matrices $\RZZ$ and $\RXX$ are upper triangular of dimension
$q\times q$ and $p\times p$ respectively.  The corresponding vectors,
$\rZy$ and $\rXy$, are of dimension $q$ and $p$, and $\ryy$ is a
scalar.  The conditions that $\bOmega$ be positive definite and $\bX$
have full column rank ensure that $\RZZ$ and $\RXX$ are nonsingular.

In our implementation we do not form the upper triangular Cholesky
factor $\RZZ$.  Instead we use Tim Davis's LDL
package~\citep{Davis:2004} to factor
\begin{equation}
  \label{eq:LDLfactor}
  \bZ\trans\bZ+\bOmega=\bL\bD\bL\trans
\end{equation}
where $\bL$ is a sparse, unit, lower triangular matrix and $\bD$ is
diagonal with positive diagonal elements. Because the diagonal
elements of the unit triangular matrix $\bL$ are, by definition,
unity, they are not explicitly stored.

In general the matrices $\bZ\trans\bX$ and $\bX\trans\bX$ are dense.
We use functions from the LDL package to solve for $\RZX$ in
\begin{equation}
  \label{eq:RZX}
  \bD^{1/2}\bL\trans\RZX=\bZ\trans\bX
\end{equation}
Having solved for $\RZX$ we can downdate $\bX\trans\bX$ and determine
the dense Cholesky factor $\RXX$ in
\begin{equation}
  \label{eq:RXX}
  \bX\trans\bX-\RZX\trans\RZX=\RXX\trans\RXX
\end{equation}
Similar relationships are used to determine $\rZy$, $\rXy$, and
$\ryy$.  In fact, in our implementation we append $\by$ to $\bX$ when
forming $\bZ\trans\bX$ and $\bX\trans\bX$ so that
(\ref{eq:denseupdate}) provides both $\RZX$ and $\rZy$ and
(\ref{eq:RXX}) provides $\RXX$, $\rXy$, and $\ryy$.

The conditional estimates of $\bbeta$ satisfy
\begin{equation}
  \label{eq:betaHat}
  \RXX\widehat{\bbeta}(\btheta)=\rXy
\end{equation}
and the conditional modes of the random effects satisfy
\begin{equation}
  \label{eq:ConditionalExp}
  \bD^{1/2}\bL\trans\widehat{\bb}(\btheta)=\rZy-\RZX\widehat{\bbeta} .
\end{equation}
The conditional ML estimate of $\sigma^2$ is
$\widehat{\sigma^2}(\btheta)=\ryy^2/n$ and the conditional REML
estimate is $\widehat{\sigma^2}_R(\btheta)=\ryy^2/(n-p)$.

The profiled optimization problem, expressed in terms of the
deviance, is
\begin{equation}
  \label{eq:ProfiledLogLik}
  \begin{aligned}
    \widehat{\btheta}&=\arg\min_{\btheta} -2\tilde{\ell}(\btheta)\\
    &=\arg\min_{\btheta}\left\{\log\left(\frac{\left|\bD\right|}
      {\left|\bOmega\right|}\right)
    + n\left[1+\log\left(\frac{2\pi\ryy^2}{n}\right)\right]\right\}
  \end{aligned}
\end{equation}
\begin{equation}
`  \label{eq:ProfiledLogRestLik}
  \begin{aligned}
    \widehat{\btheta_R}&=\arg\min_{\btheta} -2\tilde{\ell_R}(\btheta)\\
    &=\arg\min_{\btheta}\left\{\log\left(\frac{\left|\bD\right|\left|\RXX\right|^2}
      {\left|\bOmega\right|}\right)
    +  (n-p)\left[1+\log\left(\frac{2\pi\ryy^2}{n-p}\right)\right]\right\}
  \end{aligned}
\end{equation}
for ML and REML estimation, respectively.  The gradients of these
criteria are
\begin{align}
  \label{eq:gradDev}
  \nabla(-2\tilde\ell)&=\tr\left[\der\bOmega\left(
      (\bZ\trans\bZ+\bOmega)^{-1}-\bOmega^{-1}+
      \frac{\widehat{\bb}}{\widehat{\sigma}}
      \frac{\widehat{\bb}}{\widehat{\sigma}}\trans\right)\right]\\
  \label{eq:gradDevRest}
  \nabla(-2\tilde\ell_R)&=\tr\left[\der\bOmega\left(
      \vb-\bOmega^{-1}+
      \frac{\widehat{\bb}}{\widehat{\sigma}_R}
      \frac{\widehat{\bb}}{\widehat{\sigma}_R}\trans\right)\right]
\end{align}
where
\begin{equation}
  \label{eq:vbDef}
  \vb=\bL\invtrans\bD^{-1/2}\left(\bI+\RZX\RXX^{-1}\RXX\invtrans
    \RZX\trans\right)\bD^{-1/2}\bL^{-1}
\end{equation}
and $\der$ denotes the Frechet derivative.

If good starting estimates of $\btheta$ are not available, the initial
Newton iterations for (\ref{eq:ProfiledLogLik}) or
(\ref{eq:ProfiledLogRestLik}) can be unstable.  We can refine our
initial estimates with a moderate number of ECME steps
for which $\btheta_{i+1}$ satisfies
\begin{equation}
  \label{eq:theta1}
  \tr\left[\der\bOmega\left(
      \frac{\widehat{\bb}(\btheta_i)}{\widehat{\sigma}(\btheta_{i})}
      \frac{\widehat{\bb}(\btheta_i)\trans}{\widehat{\sigma}(\btheta_{i})}+
      \left(\bZ\trans\bZ+\bOmega(\btheta_{i})\right)^{-1}
      -\bOmega(\btheta_{i+1})^{-1}\right)\right]=\bzer
\end{equation}
for ML estimates or
\begin{equation}
  \label{eq:theta1R}
  \tr\left[\der\bOmega\left(
      \frac{\widehat{\bb}(\btheta_{i})}{\widehat{\sigma}_R(\btheta_i)}
      \frac{\widehat{\bb}(\btheta_{i})\trans}{\widehat{\sigma}_R(\btheta_i)}+
      \vb(\btheta_{i})
      -\bOmega(\btheta_{i+1})^{-1}\right)\right]=\bzer
\end{equation}
for REML.

At this point it is easy to formulate a general method of obtaining ML
or REML estimates for a linear mixed model:
\begin{enumerate}
\item Given the data $\by$ and the model matrices $\bX$ and $\bZ$,
  formulate initial estimates $\btheta_0$.  Some heuristics for doing
  so are given in \citet[ch.~3]{pinh:bate:2000}.
\item Use a moderate number of ECME steps, (\ref{eq:theta1}) or
  (\ref{eq:theta1R}), to refine these starting estimates.  Each ECME
  step requires evaluating $\bOmega(\btheta)$ followed by the
  decomposition (\ref{eq:CrossProdGen}) and the solutions to (\ref{eq:RZX}),
  (\ref{eq:RXX}), (\ref{eq:betaHat}) and (\ref{eq:ConditionalExp}).
\item Use a Newton method to optimize the criterion
  (\ref{eq:ProfiledLogLik}) or (\ref{eq:ProfiledLogRestLik}) with
  gradient (\ref{eq:gradDev}) or (\ref{eq:gradDevRest}).  Each
  evaluation of the criterion requires evaluating $\bOmega(\btheta)$
  followed by the decomposition (\ref{eq:CrossProdGen}) and the
  solutions to (\ref{eq:RZX}), and (\ref{eq:RXX}).  Gradient
  evaluations require the solutions to (\ref{eq:betaHat}) and
  (\ref{eq:ConditionalExp}).
\end{enumerate}
In \citet{bate:debr:2004} we show that similar calculations can be
used to evaluate the Hessian of the profiled criteria and that the
deviance forms of the criteria are bounded below throughout the
parameter space.  Reasonable starting values determined by
the ECME iterations and analytic expressions for the gradients and
Hessians help to make (\ref{eq:ProfiledLogLik}) and
(\ref{eq:ProfiledLogRestLik}) very well controlled optimization
problems.  The most difficult computational step in the ECME or Newton
iterations is the sparse Cholesky decomposition (\ref{eq:CrossProdGen}).

\section{Symbolic analysis}
\label{sec:Symbolic}

Although the decomposition (\ref{eq:CrossProdGen}) will be performed
many times for different trial values of $\btheta$, the structure of
$\bZ\trans\bZ+\bOmega$ -- in particular, the number and positions of
the non-zeros in $\bZ\trans\bZ+\bOmega$ and in $\bL$ -- will be the
same for each evaluation.  Because the LDL package provides one C
function to perform the symbolic analysis that determines the number
and position of the non-zeros in $\bL$ and another C function that
determines the numerical values in the decomposition, we can do
the symbolic analysis separately.

The number and positions of the nonzeros in $\bL$ depends on the
positions of the nonzeros in $\bZ\trans\bZ$. Any nonzero position in
the lower triangle of $\bZ\trans\bZ$ is potentially nonzero in $\bL$.
However other positions in $\bL$ can become nonzero during the course
of the decomposition.  This is called ``fill-in''.  The extent of the
fill-in can be changed by reordering the components of $\bb$ and,
correspondingly, the columns of $\bZ$.

Although there are general approaches, such as approximate minimal
degree~\citep{Davis:1996} or graph-partitioning
algorithms\citep{Metis}, for determining a fill-minimizing
permutation, it is more effective for us to exploit the special
structure of $\bZ\trans\bZ$ in searching for such a permutation.

As mentioned above, when considering the structure of
$\bZ\trans\bZ+\bOmega$ we need only consider the structure for the
variance components model because the structure for the general model
is obtained from the structure for the variance components model by
replacing each nonzero in the $(i,j)$ block of the variance components
model by a $q_i\times q_j$ nonzero matrix.  Similarly we can derive
the structure of the $\bL$ matrix for the general model from that of
the variance components model provided we restrict our attention to
permutations that do not mix levels from different grouping factors.
That is, we consider only those fill-reducing permutations that
consist of, at most, a permutation of the grouping factors and permutations of
the levels within each grouping factor.

In what follows we restrict our attention to the variance components model.

Fill-in is determined by the elimination tree~\citep{Liu:1990} for the
symmetric matrix.  We can determine the Cholesky decomposition, and
hence the elimination tree and the extent of the fill-in, column-wise
starting with the first column.  We know that there will be
``original'' nonzeros in $\bL$ wherever there are nonzeros in the
lower triangle of $\bZ\trans\bZ+\bOmega$ and, possibly, some
additional, ``induced'' nonzeros.  If there are nonzeros, either
original or induced, in column $j$ below the diagonal, say, in rows
$i$ and $k$, then a nonzero is induced in the $(i,k)$ position of
$\bL$.  Consider again the division of $\bZ\trans\bZ+\bOmega$ and
$\bL$ into a $k\times k$ array of blocks determined by the grouping
factors.  For a variance components model, the diagonal blocks are
themselves diagonal.  Because the $(1,1)$ block is diagonal the row
numbers of any nonzeros below the diagonal must be greater than $m_1$.
That is, there will not be any induced nonzeros in the first $m_1$
columns. Because this first block of $m_1$ rows and columns will not
experience any fill-in, we choose the first grouping factor to make
$m_1$ as large as possible.

There will be no need to permute the levels of the first grouping
factor.  This can be a considerable savings in the effort required to
determine a fill-reducing permutation.  For the Scottish secondary
school example we can leave the first 148 columns in their original
order and consider only permutations of the last 19 columns.

We obtain the matrix to determine the permutation of the second and
subsequent groups by ``projecting'' the first $m_1$ columns onto the
last $q-m_1$ columns as described above.  If there are only two
grouping factors this corresponds to recording the positions $(i,k)$
with $i,k>m_1$ and both $(i,j)$ and $(i,k)$ are non-zero for some
$j\le m_1$.  If there are more than two grouping factors then we
record all of these plus any of the original nonzeros below the
$m_1$st row and to the right of the $m_1$st column.

In the case of two nested grouping factors there will only be one
nonzero element below the diagonal in the first $m_1$ columns, hence
there is no fill-in.  For more than two nested grouping factors, any
pair of nonzeros occuring in the first $m_1$ columns must be in rows
that correspond to different grouping factors and that off-diagonal in
the projected matrix must already be nonzero.  That is, nested
grouping factors do not generate any fill-in.  Not only can the matrix
$\bL$ be created ``in place'' (that is, with exactly the same
positions of nonzero off-diagonal elements as in $\bZ\trans\bZ$) but
also $\bL^{-1}$ has the same pattern of nonzeros.  This is unusual.
In most cases $\bL^{-1}$ has many more nonzeros than does $\bL$.

Notice that a single grouping factor is, trivially, a nested sequence.


\subsection{Examples}
\label{sec:ProjectionEx}

In Figure~\ref{fig:ScotsProj} we show the projection of the pairwise
crosstabulation of \var{primary} and \var{second} in the
\var{ScotsSec} data, as shown in Figure~\ref{fig:ZtZ}, onto the
$19\times19$ block for the \var{second} factor.
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<FigScotsProj,fig=TRUE,echo=FALSE,height=4,width=10>>=
proj = .Call("sscCrosstab_project2", ctab)
trp1 = as(proj, "tripletMatrix")
trp2 = as(.Call("sscMatrix_ldl_symbolic", proj, FALSE)[[2]], "tripletMatrix")
symb = .Call("sscMatrix_ldl_symbolic", proj, TRUE)
## use the permutation to create the permuted symmetric matrix
iperm = perm = symb[[3]]
iperm[perm+1] = seq(a = perm) - as.integer(1)
trp3 = trp1
trp3@i = iperm[trp1@i + 1]
trp3@j = iperm[trp1@j + 1]
trp4 = as(symb[[2]], "tripletMatrix")
df <- data.frame(i = c(trp1@i, trp2@i, trp3@i, trp4@i),
                 j = c(trp1@j, trp2@j, trp3@j, trp4@j),
                 x = c(trp1@x, trp2@x, trp3@x, trp4@x),
                 type = factor(rep(c("Projected", "L", "Permuted", "L-perm"),
                 c(length(trp1@x), length(trp2@x), length(trp3@x),
                   length(trp4@x))),
                 levels = c("Projected", "L", "Permuted", "L-perm")))
df$x[] = 0.1
df$x[df$i == df$j] = 1
print(levelplot(x ~ j + i | type, data = df,
                sub = "Dimensions: 19 x 19",
                xlim = c(-0.5, 18.5), ylim = c(18.5, -0.5), xlab = "Column",
                ylab = "Row", aspect = 'xy',
                col.regions = grey(seq(from = 0.7, to = 0, length = 100)),
                par.settings = list(background = list(col = "transparent")),
                colorkey = FALSE, layout = c(4,1)))
@   
  \caption{Projection of the pairwise crosstabulation for the Scottish
    secondary school data onto the $19\times 19$ lower right block for
    the \var{second} factor.  The ``Projected'' panel shows the
    original (black squares) and projected (gray squares) non-zero
    positions.  The ``L'' panel shows the implicit diagonal (black
    squares) and the non-zero off-diagonal (gray squares) for this
    ordering of the levels of the factor.  The ``Permuted'' panel
    shows the symmetric matrix after permuting the rows and columns
    according to the fill-reducing permutation determined by the Metis
    package and the ``L-perm'' panel shows the $\bL$ matrix from this
    ordering.}
  \label{fig:ScotsProj}
\end{figure}

%% \section{Generalizations of linear mixed models}
%% \label{sec:Generalizations}

%% \section{Further enhancements}
%% \label{sec:Further}


\section{Acknowledgements}
\label{sec:Ack}

This work was supported by U.S.{} Army Medical Research and Materiel
Command under Contract No.{} DAMD17-02-C-0119.  The views, opinions
and/or findings contained in this report are those of the authors and
should not be construed as an official Department of the Army
position, policy or decision unless so designated by other
documentation.

I thank Deepayan Sarkar and Tim Davis for helpful discussions and
suggestions and Harold Doran for his suggestion of using sparse
matrix techniques for linear mixed-effects models.

The \code{Matrix} package for \RR{} is based on code from the
LDL~\citep{Davis:2004}, TAUCS~\citep{Taucs}, and Metis~\citep{Metis}
packages.

\bibliography{lme4}
\end{document}
Yesterday I spoke with Deepayan Sarkar, a graduate student with whom I
work, on ways for structuring the calculations for linear
mixed-effects models using sparse matrix representations and Tim
Davis's LDL decomposition code.  I described to Deepayan a plan A and
a plan B.  On thinking about the calculations more yesterday evening I
formulated a plan AB, which is what I think I will use.

One purpose of writing this note is so I can get the steps clear in my
mind.

The critical part of the calculation is determining the Cholesky
decomposition of a matrix of the form

 Z'Z+W Z'X Z'y
  X'Z  X'X X'y   for different matrices W that depend upon parameters r
  y'Z  y'X y'y

Statistically the role of y is very different from X but, as far as
the calculation goes, I can work with the augmented matrix [X,y] as if
it were a single matrix, which, at the risk of some confusion, I will
henceforth call X.  We will not refer to y again.

Matrices Z and X are n by q and n by p respectively with n >= q.
Generally, q >> p, Z is sparse, W is block diagonal (and the blocks
are of small dimension) and X is dense.  The matrix Z is generated
from a set of k grouping factors, each of length n, and corresponding
model matrices M1, M2, ..., Mk where Mi is n by qi.  The i'th grouping
factors, gi, only assume values in the range 1,...,mi and takes each
value in that range at least once.

Generally the qi, i = 1,...,k are very small (values of 1 or 2 are
common) but the mi can be large.  In the Scottish secondary school
data the grouping factors are the primary school attended and the
secondary school attended with m1=148 and m2=19 levels respectively.
Each observation corresponds to a student. The total number of
observations is n = 3435.  We would usually start our modeling with a
so-called ``variance components'' model for which q1 = q2 = 1 and
M1(=M2) is the 3435 by 1 matrix, all of whose entries are 1.  The
corresponding 167 by 167 matrix W will consist of two diagonal blocks
of sizes 148 by 148 and 19 by 19 respectively where each of these
blocks is a (positive) multiple of an identity matrix.

The matrix X has at least 2 columns (recall that the response is the
rightmost column of this X).  In keeping with the design of the LDL
package we will store the upper triangle of Z'Z as a compressed,
sparse, column-oriented matrix and Z'X and X'X as dense matrices.

In the variance components model the matrix Z is the concatenation of
k groups of columns, where the i'th group of columns is the mi
indicator columns for grouping factor gi.  In the Scottish secondary
students data Z = [Z1, Z2] where Z1, of size 3435 by 148, is the
indicators of the primary school for each student and Z2, of size 3435 by
19, is the indicators of the secondary school.  Matrices Z1'Z1 and
Z2'Z2 are diagonal with non-negative integers (the number of students
who attended that school) on the diagonal.  The matrix Z1'Z2 can be sparse.

Nested grouping factors:

If the levels of g1 are nested within the levels of g2 then each
column of Z1'Z2 has exactly one non-zero entry.  That is, each level
of g1 occurs with exactly one level of g2.  If the grouping factors
form a nested sequence, in the sense that gi is nested within gi+1 for
i = 1,...,k-1, then Z'Z has an especially simple structure in that the
Cholesky decomposition does not generate any ``fill-in''.  That is,
the Cholesky decomposition of Z'Z+W can be calculated in place and the
Cholesky factor can be inverted in place.  Because nested sequences of
grouping factors do not generate any fill-in it is unnecessary to
search for a fill-minimizing permutation of the levels of the factors.

A single grouping factor trivially forms a nested sequence of grouping
factors.

We detect and exploit this structure when it is present.

Pairwise cross-tabulation:

We will refer to the Z'Z matrix for the variance components form of
the model as the pairwise cross-tabulation of the factors.  This
matrix can be used to check for nested grouping factors and to
calculate fill-reducing permutations for non-nested factors.  Even
when some of the model matrices associated with grouping factors have
multiple columns, the pattern of non-zero elements in Z'Z, and the
fill-reducing permutation of the levels within the groups can be
determined from the pairwise cross-tabulation.

Our general algorithm is:

  determine the pairwise cross-tabulation of the grouping factors
  check for a sequence of nested grouping factors (trivially
    satisfied by a single grouping factor)
  if (non-nested) {
     determine fill-reducing permutation
     separate the groups within this permutation
  }
  if (any qi > 1) {
     re-evaluate Z'Z
  }
  create Z'X and X'X
  use ldl_symbolic to determine Lp (and hence the size of Li and Lx)

  given the diagonal blocks of W
    form Z'Z+W from a copy of Z'Z
    ldl_symbolic of Z'Z+W
    solve LY = Z'X for Y
    Cholesky decomposition (dpotrf) of X'X-(Z'X)'D^{-1}Y
    save D^{-1}Y


  
  The ordering of the columns (and, correspondingly, the rows) of a
  positive semidefinite, symmetric sparse matrix can have a
  substantial effect on the amount of fill-in generated by the
  Cholesky decomposition.  For the particular matrices considered here
  the ordering will only become important when random effects are
  associated with more that one grouping factor and the grouping
  factors are neither nested nor fully crossed.  We say that such
  factors are partially crossed, a situation that is very common in
  observational data.

  Several methods for determining favorable orderings have been
  proposed but these generally reorder all the columns.  In our case
  the columns are grouped.  We show that we can reorder the columns
  while preserving the grouping and still attain acceptable levels of
  fill-in. 


This report is directed at two audiences: sparse matrix researchers
and mixed-effects model researchers.  To make the ideas accessible to
both audiences I will need to introduce a formulation of mixed models
for the sparse matrix researchers and also to introduce sparse matrix
representation for the mixed model researchers.


Opportunities for efficiency exist because we must repeatedly perform
Cholesky decompositions of matrices with the same pattern of non-zero
entries (and somewhat different values of those non-zero entries) and
because the non-zero entries occur in dense blocks.  Many methods for
sparse matrices have both a symbolic phase, where the pattern of the
non-zero entries in the result is determined, and a numeric phase,
where the numerical results are determined.  For decompositions part
of the symbolic phase is determination of permutation of the rows and
columns that reduces fill-in for the decomposition.  In our problem
the symbolic phase need only be done once and it can be performed
based the positions of the blocks.  The matrix giving the positions of
the blocks is frequently much smaller and easier to manipulate than
the matrix that is to be decomposed.  Furthermore, the blocks provide
a natural way of using supernodes that employ dense matrix building
blocks in a sparse matrix calculation, for this problem.

In the next section I introduce a general form of linear mixed-models
and the notation I will use.  I also introduce three sample data sets
and models.  These include a very simple example that can be used to
illustrate details of the calculations, a moderate-sized example that
has been used to illustrate other methods, and a very large example
that can show the savings available with sparse matrix methods.  In
\S\ref{sec:Examples} I introduce sparse matrix storage schemes and
computational methods and show how they can be applied to the
examples. 


Using the response and the model
matrices, $\by$, $\bX$ and $\bZ$, which are evaluated from the
observed data, we determine the estimates of the model parameters;
$\bbeta$, $\btheta$, and $\sigma^2$, as those values that optimize the
likelihood function or, more commonly, a variant called the restricted
likelihood.  Both the likelihood and the restricted likelihood must be
positive and their logarithms, called the log-likelihood and
the log-restricted-likelihood, are generally easier to evaluate and
provide a better quadratic approximation for optimization than the
original functions, so we work on the log-likelihood scale.






When the random effects are grouped according to more than one
grouping factors, as here, it is important to determine if the
grouping factors are \emph{crossed} (every level of factor 1 occurs
with every level of factor 2) or nested (each level of factor 1 occurs
with only one level of factor 2) or partially crossed, which is how we
describe grouping factors that are neither (fully) crossed nor
strictly nested.

In this case the grouping factors \var{primary} and \var{second} are
partially crossed.  We can express this graphically through the image
of the cross-tabulation of the grouping factors.  We can generate such
a cross-tabulation as a symmetric, sparse, compressed column matrix
with the \var{sscCrosstab} function and use \var{image} to show the
non-zero elements graphically.  (Only the non-zero elements in the
lower triangle are shown.)
<<ctab>>=
ctab = sscCrosstab(ScotsSec[, c("primary", "second")])
str(ctab)
@ 


\subsection{A simple variance components model}
\label{sec:varianceComponents}

In \citet[\S1.1]{pinh:bate:2000} we discuss measurements of the travel
time of a certain type of ultrasonic wave in six different
railway rails.  Each rail was tested three times yielding a total of
18 observations.  Each observation denotes the rail and the observed
travel time. A simple data plot (e.g.{} Fig. 1.1 in
\cite{pinh:bate:2000}) shows that the variation between responses on
different rails is much greater than the variation between responses on
the same rail.  We model this as
\begin{equation}
  \label{eq:varianceComp}
  y_{ij}=\mu+b_i+\epsilon_{ij}\quad
  i=1,\dots,6,\;j=1,\dots,3\quad
  b_i\sim\mathcal{N}(0,\sigma^2_b),\;\epsilon_{ij}\sim\mathcal{N}(0,\sigma^2)
\end{equation}
where $\sigma^2$ is the within-rail variance and $\sigma^2_b$ is the
between-rail variance.  These are called the \emph{variance components}.

The random variable $b_i$ is the deviation of the mean travel time for
rail $i$ from the overall mean travel time $\mu$.  These are called
the \emph{random effects} associated with the rails.

We can express model (\ref{eq:varianceComp}) in the form
(\ref{eq:lmeGeneral}) by setting
\begin{equation}
  \label{eq:varianceCompMod}
  \begin{aligned}
    \bb &=\left(b_1,b_2,\dots,b_6\right)\trans\\
    \bbeta &= \mu
    \bX\trans&=\left[1,1,\dots,1\right]\trans
  \end{aligned}
\end{equation}
and $\bZ$ to be the $18\times 6$ matrix of indicators of the rail.
The matrix 
\begin{equation}
  \label{eq:relativePrecision}
  \bOmega=\frac{\sigma^2}{\sigma_a^2}\bI_6
\end{equation}
where $\bI$ is the $6\times 6$ identity matrix.  The multiple
$\frac{\sigma^2}{\sigma_a^2}$ is the relative precision of the random
effects and the parameter $\theta$ is a scalar that determines this
multiple.  To obtain an unconstrained $\theta$ we could use the
logarithm of the ratio
\begin{equation}
  \label{eq:thetaDef}
  \theta = \log\left(\sigma^2\right)-\log\left(\sigma_a^2\right)
\end{equation}

The first few rows of $\bZ$, $\bX$, and $\by$ are
<<ZXy>>=
data(Rail, package = "nlme")
ZXy = cbind(model.matrix(~Rail-1,Rail), X = 1, y = Rail$travel)
ZXy[1:4,]
@ 
The matrix to be decomposed is obtained by adding $\bOmega$ to the
$6\times 6$ upper left submatrix of
<<crossprod>>=
crossprod(ZXy)
@ 
For example, if $e^\theta=0.1$ then the Cholesky decomposition is
<<chol1>>=
options(digits=5)
chol(crossprod(ZXy) + diag(c(rep(.1, 6), 0, 0)))
@ 

As seen in this example the cross-product matrix is sparse and only
only the diagonal and the last two rows need to be stored.  (It
happens that $\bZ\trans\bZ$ is a multiple of the identity, as is
$\bOmega$, and this could lead to further simplifications.  However,
this property depends on the fact that the data are balanced in the
sense that there are the same number of observations made on each
rail.  Although data from a designed experiment may be balanced,
observational data are almost never balanced so it is not worthwhile
trying to exploit this special structure.)

In general the trailing $p+1$ rows (and columns) of the cross-product
matrix will be dense.  The structure of the other 
summarized as


In \S\ref{sec:SparseM} we describe two of the popular sparse
matrix representations and formation of the Cholesky decomposition of
sparse, symmetric, positive semidefinite matrices.  The number of
non-zero entries in the Cholesky factor can depend on the ordering of
the columns (and, correspondingly, the rows) of the original matrix.
Various methods have been proposed to choose optimal or near-optimal
reorderings.  This is an example of symbolic analysis that can be used
before the numeric computation to reduce the amount of numeric
computation.  We describe others.


\section{Sparse matrix classes and methods in the Matrix package for R}
\label{sec:SparseM}

The simplest representation of a sparse matrix $\bX$ is to store a
triplet $(i,j,x_{ij})$ for each non-zero element.  If the triplets are
sorted, say by column order, the column indices will occur in blocks
of equal values.  In the \emph{compressed, sparse, column-oriented}
format the entries are sorted in increasing column order and a set of
pointers to the beginning of each column are used instead of the
column values themselves.  The \var{tripletMatrix} class in the
\var{Matrix} package provides the triplet format and the
\var{cscMatrix} class provides the compressed, sparse column-oriented
format.  In both these classes indices are 0-based (for compatibility
with the underlying C code) and not 1-based as is common in R.
<<Matintro>>=
library(Matrix)
mm = new("tripletMatrix", 
         i = as(c(0,2,3,1,2,0,3,4,3,4),"integer"),
         j = as(c(0,0,0,1,1,2,2,2,3,3),"integer"),
         x = (1:10)/10, Dim = as(c(5,4),"integer"))
m1 = as(mm,"cscMatrix")
str(m1)
as(m1, "matrix")
diff(m1@p)
@ 
We see that the \var{p} slot in a \var{cscMatrix} with 4 columns has 5
elements.  The first element is always zero and the successive differences
are the numbers of non-zero elements in each column.  The total number
of non-zero elements is the value of the last element of the \var{p}
slot.  This is also the length of the vector of row indices in the
\var{i} slot.

The validation method for the \var{cscMatrix} class ensures that the
row indices are increasing within columns and reorders the \var{i} and
\var{x} slots if necessary to achieve this.  Technically, the objects
in this class can be described as sorted, compressed, sparse,
column-oriented matrices.

Objects in the \var{tscMatrix} class represent triangular, sparse,
column-oriented matrices and those in the \var{sscMatrix} class
represent symmetric, sparse, column-oriented matrices.  Only the
upper triangle or the lower triangle, as indicated by \code{"U"} or
\code{"L"} in the \var{uplo} slot, of a symmetric matrix is stored.
The \var{crossprod} function applied to a \var{cscMatrix} produces an
\var{sscMatrix}.
<<crossprod>>=
class(m2 <- crossprod(m1))
as(m2, "matrix")
@

In Statistics we usually define the Cholesky decomposition of a
positive semidefinite, symmetric matrix $\bA$ as an upper triangular
matrix $\bR$ such that $\bA=\bR\trans\bR$ but it is also frequently
defined as a lower triangular matrix $\bL$ such that
$\bA=\bL\bL\trans$.  Naturally, $\bL$ and $\bR$ are transposes of each
other.  On occasion there are advantages to working with the left
factor $\bL$ instead of the right factor $\bR$.

For a sparse, symmetric, semidefinite matrix reordering the columns
(and, correspondingly, the rows) of $\bA$ can change the number of
non-zero elements in the Cholesky factor.  The number of elements in
the Cholesky factor is at least the number of non-zero elements in the
lower triangle of $\bA$.  Additional non-zeros can be generated during
the decomposition.  This process is called ``fill-in''.  Various
methods of determining a fill-minimizing order have been proposed.  We
use a graph-based method implemented in the Metis package.

The \var{chol} function generates the Cholesky decomposition.   When
applied to an \var{sscMatrix} object it defaults to generating a
fill-reducing permutation and the Cholesky factor of the permuted matrix.
<<chols>>=
m3 = chol(m2)
as(m3, "matrix")
m3@perm
@ 
If we set the optional argument \var{pivot} to \var{FALSE},
calculation of the fill-reducing permutation is suppressed.
<<chols2>>=
as(chol(m2, pivot = FALSE), "matrix")
@ 
In this example the fill-reducing permutation reverses the order of
the columns and rows of \var{m2} before taking the decomposition.  It
results in two fewer non-zero elements in the decomposition than when
we suppress the permutation.

\subsection{Symbolic versus numeric computation}
\label{sec:symbolic}

Calculation of the fill-reducing ordering is an example of a symbolic
computation on sparse matrices in that it is based only on the
positions of the non-zero elements, not upon their values.  Frequently
a sparse-matrix computation has both a symbolic phase, which typically
determines the number and positions of the non-zero entries in the
result, and a numeric phase that actually calculates these non-zero
elements.

Evaluation of the profiled log-likelihood or profiled
log-restricted-likelihood requires updating the diagonal blocks in
$\bZ\trans\bZ$ and taking the Cholesky decomposition of the resulting
matrix.  The symbolic phases, including calculation of a fill-reducing
ordering only need to be done once.

Recently Tim Davis has released the LDL package that provides a
concise Cholesky factorization of the form $\bA=\bL\bD\bL\trans$ where
$\bL$ is a unit lower triangular matrix (i.e. all the diagonal
elements are unity) and $\bD$ is diagonal and stored as a single
vector.  This representation is particularly convenient for us because
the diagonal elements (which must be non-zero when $\bA$ is positive
definite) often constitute a substantial portion of the total number
of non-zero elements in the Cholesky factor and, in this
representations, we do not encounter the extra indexing overhead when
accessing these elements.  Also the determinant of $\bA$ (or, for our
purposes, the determinants of leading diagonal submatrices of $\bA$)
can be calculated directly from the diagonal of $\bD$.

As shown in the examples in the next section we can determine
fill-reducing orderings and sizes of Cholesky factors of the matrices
that we wish to decompose by considering first the pairwise
cross-tabulations of the grouping factors.


\begin{equation}
  \label{eq:qtot}
\end{equation}
Generally $p$, the dimension of $\bbeta$, is moderate but $q$, the
dimension of $\bb$, and $n$, the number of observations in $\by$,
which is also the number of rows in $bX$ and $\bZ$, can
be extremely large.  The $n\times q$ matrix $\bZ$ and the $q\times q$
matrices $\bZ\trans\bZ$ and $\bOmega$ can be prohibitively large but
they are sparse (i.e. most of the elements of these matrices are zero)
and, especially in the case of $\bOmega$, highly structured.

The dimension of $\btheta$ is typically very small.  Even in complex,
``real-world'' models applied to data sets where $n$ can be in the
millions, the dimension of $\btheta$ can be as small as three or less.
As described in \citet{bate:debr:2004}, the general problem we
consider is determining the values of the parameters $\bbeta$,
$\btheta$, and $\sigma^2$, in model (\ref{eq:lmeGeneral}) that
optimize the likelihood function, providing the maximum likelihood or
ML estimates, or, more commonly, a modified version called the
restricted likelihood, providing the REML estimates. 

Sparsity in $\bZ$, $\bZ\trans\bZ$ and $\bOmega$ occurs when the random
effects vector $\bb$ is divided into small components associated with
$k$ factors that group the observations and the random effects
associated with each grouping factor are independent between groups
and are i.i.d. (independent and identically distributed) within
groups.  The size of the components in each group is $q_i$ and the
number of distinct levels of each grouping factor is $m_i, i =
1,\dots,k$.  Then $\bOmega$ is block diagonal with $k$ blocks and
block $i$ is itself block diagonal with the diagonal consisting of
$m_i$ repetitions of a $q_i\times q_i$ matrix $\bOmega_i$.  


In common models for such data
\citep{Rnews:Lockwood+Doran+Mccaffrey:2003}, the random effects are
associated with the primary school and the secondary school that the
student attended, which is to say that \var{primary}, with 148
distinct levels, and \var{second}, with 19 distinct levels, are the
grouping factors.  


We see that $\Omega$ has a very simple structure.  The matrix
$\bZ\trans\bZ$ is sparse but not as simple.  To determine its
structure we must first examine $\bZ$ which has 3435 rows
(corresponding to students) and 167 columns.  The first 148 columns
are the indicators of the primary schools.  That is, if student $i$
attended primary school $j$ then
\begin{equation}
  \label{eq:bZik}
  \{\bZ\}_{ik}=
  \begin{cases}
    1&k=j\\
    0&k\ne j
  \end{cases}
\end{equation}
The last 19 columns are the indicators of the secondary schools.
The diagonal elements of $\bZ\trans\bZ$ are the tabulation of the
number of students attending each primary school followed by the
number of students attending each secondary school.  Part of this
tabulation is given in the output of the \var{summary} function above.
Here we give the full tabulation of the secondary schools and part of
the crosstabulation of the primary and secondary schools.
<<xtabs>>=
xtabs(~second, ScotsSec)
@ 

For the purposes of this study each student is classified as attending
only one primary school and one secondary school.  Primary school $i$
can occur with secondary school $j$ but not all of these combinations
do 
one prim
but one of these columns, the column corresponding to the primary school
There is a corresponding grouping of the columns of the matrix $\bZ$
into $m_1\times q_1$ columns associated with the first grouping factor
$\bff_1$, $m_2\times q_2$ columns associated with the second grouping
factor $\bff_2$, and so on.  A given observation, corresponding to a
row of $\bZ$, is associated with exactly one level of $\bff_1$, one
level of $\bff_2$, and so on and all the elements of that row of $\bZ$
will be zero except for the corresponding $q_1$ columns in the first
group, the corresponding $q_2$ columns in the second group, and so on.

This structure in $\bZ$ induces a special structure in $\bZ\trans\bZ$.
It is symmetric and can be divided into $k\times k$ ``outer blocks'',
corresponding to the $k$ grouping factors. The $(i,j)$ outer block, of
size $m_i q_i\times m_j q_j$, is further subdivided into $m_i\times
m_j$ inner blocks, each of size $q_i\times q_j$.  The $(m,n)$'th inner
block in the $(i,j)$'th outer block will be non-zero only if level $m$
of $\bff_i$ occurs with level $n$ of $\bff_j$.  In particular, a
diagonal outer block is block-diagonal with the diagonal consisting of
$m_i$ (possibly different) blocks of size $q_i\times q_i$.

As discussed later, $\bOmega$ has a block diagonal structure and its
determinant is easily evaluated.  The determinant
$\left|\bZ\trans\bZ+\bOmega\right|$ is the product of the elements of
$\dZ$ and $\left|\LXX\right|^2$ is the product of the elements of $\dX$.

The other results given in \citet{bate:debr:2004} can be calculated
from $\bL$ and $\bD$.  To make all this feasible the structure and, in
particular, the sparsity of $\bZ$ and $\bOmega$ must be exploited.

Sparsity in $\bZ$ (and $\bOmega$) occurs when the random effects
vector $\bb$ is divided into small components associated with one or
more factors that group the observations.  It is easiest to illustrate
this with some examples.

\subsection{Examples}
\label{ssec:Examples}

Data on achievement scores of Scottish secondary school students,
as described in \citet{Paterson:1991} and \citet[ch.~18]{MLwiN:2002},
are available as the data set \var{ScotsSec}. 

This data set contains the achievement scores (\var{attain}), some
demographic data (sex and social class), a verbal reasoning score
based on tests taken at entry to secondary school, and the primary and
secondary schools attended for 3435 students.  There are 148 distinct
primary schools and 19 distinct secondary schools represented in these
data.
<<ScotsSec>>=
data(ScotsSec, package = "Matrix")
dim(ScotsSec)
summary(ScotsSec[,c("attain", "verbal", "sex", "primary", "second")])
@ 

In common models for such data
\citep{Rnews:Lockwood+Doran+Mccaffrey:2003}, there would be one or
more coefficients in $\bb$ associated with each school (i.e.
\var{primary} and \var{second} are the grouping factors for the random
effects).  Let's start with a simple model in which the random effects
for the primary school and the random effects for the secondary school
are both simple additive effects.  This means that the first 148
columns of $\bZ$ are indicators of the primary school and the last 19
columns are indicators of the secondary school.  We incorporate the
student's verbal score and sex and their interaction in the fixed
effects part of the model (the model matrix $\bX$ and the coefficients
$\bbeta$).

<<fm1>>=
fm1 = lme(attain~verbal*sex,ScotsSec,list(primary=~1,second=~1))
@ 


only on the the corresponding information on
$\bZ\trans\bZ$ does not change for different values of A symbolic analysis function to determine 
from is provided in the LDL package.  A nonzero element of
$\bZ\trans\bZ+\bOmega$ The number of the
nonzeros in $\bL$ is at least as large as the number of nonzeros in
the lower triangle of , and usually larger. can change if the rows and columns of
 are permuted.  




For each value of $(\omega_1,\omega_2)$, code from the LDL package is
used to factor
\begin{equation}
  \label{eq:LDLdef}
  \bZ\trans\bZ+\bOmega=\bL\bD\bL\trans
\end{equation}
where $\bL$ is a unit, lower triangular $q\times q$ matrix and $\bD$
is diagonal with positive diagonal elements.

The number of nonzero offdiagonal elements in $\bL$ depends on the
order of the rows and columns in $\bZ\trans\bZ+\bOmega$ because this
ordering determines the amount of ``fill-in'' that occurs during the
course of the decomposition algorithm.  There are several different
approaches to determining a fill-reducing permutation of the rows and
columns.  We use an algorithm based on graph partitioning and
implemented in Metis~\citep{Metis}.

A general fill-reducing permutation algorithm will not preserve
separation of the rows (and columns) associated with different
grouping factors.  Especially when working with more general
mixed-effects models for which the $q_i,i=1,\dots,k$ can be greater
than 1, there are substantial advantages in maintaining separation of
the grouping factors.  To avoid the undesirable intermingling of the
grouping factors, we take the permutation returned by Metis and
reorder it to separate the factors.

  Terms in the Hessian can
be expressed as
\begin{align}
  \label{eq:HessDev}
  \derj\deri(-2\tilde{\ell})&=\tr\left[\derj(\deri\bOmega)\left(
      (\bZ\trans\bZ+\bOmega)^{-1}-\bOmega^{-1}+
      \frac{\widehat{\bb}}{\widehat{\sigma}}
      \frac{\widehat{\bb}}{\widehat{\sigma}}\trans\right)\right]\\
  \nonumber
  &\quad-\tr\left[\derj\bOmega
    (\bZ\trans\bZ+\bOmega)^{-1}\deri\bOmega(\bZ\trans\bZ+
    \bOmega)^{-1}\right]\\
  \nonumber
  &\quad+\tr\left(\derj\bOmega\bOmega^{-1}\deri\bOmega\bOmega^{-1}\right)
  -2\frac{\widehat{\bb}}{\widehat{\sigma}}\trans\derj\bOmega\vb\deri\bOmega
  \frac{\widehat{\bb}}{\widehat{\sigma}}\\
  \nonumber
  &\quad-\frac{1}{n}
  \left(\frac{\widehat{\bb}}{\widehat{\sigma}}\trans\derj\bOmega
    \frac{\widehat{\bb}}{\widehat{\sigma}}\right)
  \left(\frac{\widehat{\bb}}{\widehat{\sigma}}\trans\deri\bOmega
    \frac{\widehat{\bb}}{\widehat{\sigma}}\right)\\
  \label{eq:HessDevRest}
  \derj\deri(-2\tilde{\ell_R})&=\tr\left[\derj(\deri\bOmega)\left(
      \vb-\bOmega^{-1}+
      \frac{\widehat{\bb}}{\widehat{\sigma}_R}
      \frac{\widehat{\bb}}{\widehat{\sigma}_R}\trans\right)\right]\\
  \nonumber
  &\quad-\tr\left[\derj\bOmega \vb\deri\bOmega\vb\right]\\
  \nonumber
  &\quad+\tr\left(\derj\bOmega\bOmega^{-1}\deri\bOmega\bOmega^{-1}\right)
  -2\frac{\widehat{\bb}}{\widehat{\sigma}_R}\trans\derj\bOmega\vb\deri\bOmega
  \frac{\widehat{\bb}}{\widehat{\sigma}_R}\\
  \nonumber
  &\quad-\frac{1}{n-p}
  \left(\frac{\widehat{\bb}}{\widehat{\sigma}_R}\trans\derj\bOmega
    \frac{\widehat{\bb}}{\widehat{\sigma}_R}\right)
  \left(\frac{\widehat{\bb}}{\widehat{\sigma}_R}\trans\deri\bOmega
    \frac{\widehat{\bb}}{\widehat{\sigma}_R}\right)
\end{align}
where $\deri$ and $\derj$
represent differentiation with respect to elements $i$ and $j$ of
$\btheta$, respectively.

When the grouping factors are not in a strictly nested sequence, the
number of nonzeros in $\bL^{-1}$ can be much greater than the number
of nonzeros in $\bL$.  The positions of the nonzeros in the last 19
rows of $\bL^{-1}$ (again, the first 148 rows are an identity matrix
augmented with 19 columns of zeros) are shown in
Figure~\ref{fig:LInonz}.
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<FigLInonz,fig=TRUE,echo=FALSE,height=2.5,width=8>>=
fmr = fm1@rep
#LI = as(new("tscMatrix", i = fmr@LIi, p = fmr@LIp, Dim=fmr@Dim,
#         x = fmr@LIx, uplo = "L", diag = "U"), "tripletMatrix")
#print(image(LI, ylim=c(166.5,147.5), xlim=c(-0.5,166.5),
#            colorkey=FALSE,aspect='xy'))
@   
  \caption{Location of non-zero elements in the last 19 rows of the unit
    lower triangular matrix $\bL^{-1}$ for model \var{fm1} fit to the
    \var{ScotsSec} data. Darker squares indicate larger magnitudes.
    Rows and columns are numbered from zero. The levels of the
    grouping factors have been rearranged according to a fill-reducing
    permutation.}
  \label{fig:LInonz}
\end{figure}
There are 1544 nonzero off-diagonal elements in $\bL^{-1}$ for this
example.  However, even this count is less than the number of cells
required in MLwiN (2812, according to \citet{MLwiN:2002}) to
store the off-diagonal part of $\bZ\trans\bZ$.




 described in , are
a widely-used class of statistical models.  In chapter 3 of that book
we provide details of computational methods suitable for mixed models
with a single grouping factor for the random effects or with multiple,
nested grouping factors.  Those methods are based on
orthogonal-triangular (or ``QR'') decompositions.  Later, in
\citet{bate:debr:2004} we generalized and extended these QR-based
methods to provide the gradient of the criteria that are optimized by
the parameter estimates.

In this report we formulate a general approach to linear mixed model
calculations using sparse matrix methods.  For an important class
of mixed model problems, those with partially crossed grouping
factors, the sparse matrix methods are superior to any existing
methods in terms of the amount of storage required and computational
speed.  Even for the problems with simpler structure, either a single
grouping factor or nested grouping factors, an efficient
implementation of the sparse-matrix-based methods is competitive with
the best current methods.

In \S\ref{sec:LinearMixed} we review some of the results from
\citet{bate:debr:2004} to establish the basic operations to be
performed and we introduce a moderate-sized example with partially
crossed random effects.  Details of the implementation are considered
in \S\ref{sec:Implementation} and opportunities for further
enhancements are considered in \S\ref{sec:Further}.  Throughout we
illustrate these methods with the implementation provided by the
\code{lme4} and \code{Matrix} packages for R~\citep{R-1.9.0}.

 R's summary
of these variables each give the counts for the six schools with the
greatest number of students in the study.  A cross-tabulation of
\var{primary} and \var{second} (we show only the first five rows),
shows that sometimes (e.g.{} primary schools 1 and 4)
students from the same primary school attend different secondary
schools.

<<ScotsSecsummary>>=
summary(ScotsSec[,c("primary", "second")])
xtabs(~primary + second, ScotsSec)[1:5,]
@ 


General techniques for choosing a fill-reducing permutation, such as
graph-based techniques~\citep{Metis} or approximate minimal
degree~\citep{Davis:1996}, search for an arbitrary permutation.  For our
purposes there is a substantial advantage in choosing a fill-reducing
permutation that preserves separation of columns associated with
different grouping factors.  In particular, we can determine suitable
permutations based only on the grouping factors and their pairwise
crosstabulation.  That is, all we need to do is permute the levels of
each of the grouping factors based upon the positions of the nonzeros
in the pairwise crosstabulation.


We begin with a variance components model that incorporates the
student's sex, their verbal pre-test score, and the interaction 
of these two covariates in the fixed effects.
<<fm1>>=
fm1 = lme(attain~verbal*sex, data = ScotsSec, random = list(primary=~1,second=~1))
@ 


The matrix $\bZ\trans\bZ$ is stored in the compressed, sorted, sparse,
symmetric, column-oriented representation.  Only the upper triangle or
the lower triangle of this symmetric matrix need be stored.  Because
we will use the methods implemented in Tim Davis' LDL package
\citep{Davis:2004} which requires the upper triangle to be stored, we
do so.  There are a total of 404 non-zero elements in the upper
triangle.  We store these in increasing column order and, within each
column, in order of increasing row index.  In addition to the non-zero
value we must store the row indices and the column indices.  However,
the column indices are in increasing order and we can compress this
vector by storing only the information on where each column
begins. This compressed, sparse column-oriented (CSC) representation
is a standard representation for sparse matrices, as described in
\citet{Davis:2004}.

The matrix $\bOmega$ is implicitly defined by the number of levels in
each of the grouping factors and by the values of $\omega_1$ and
$\omega_2$ (shown below).  We store the matrix $\bZ\trans\bX$ and the
vector $\bZ\trans\by$ as a single dense matrix of size $167\times 5$.
Similarly $\bX\trans\bX$, $\bX\trans\by$ and $\by\trans\by$ are stored
in a single dense matrix of size $5\times 5$ (only the upper triangle
of this symmetric matrix is stored).
<<fm1Pieces>>=
unlist(fm1@rep@Omega)
dim(fm1@rep@ZtX)
fm1@rep@ZtX[1:4,]
fm1@rep@XtX
@ 

\section{Symbolic analysis of pairwise crosstabulations}
\label{sec:Symbolic}

Given the CSC form of $\bZ\trans\bZ+\bOmega$ a function in the LDL
package can compute the diagonal of $\bD$ as a vector of length
$q$ and the strict lower triangle of $\bL$ in CSC format.  This will
be done many times for different values of $\btheta$.  The key to the
computational method is determining $\bL$ and, when feasible,
$\bL^{-1}$ efficiently.

The number and positions of the nonzeros in $\bL$ depends on the
positions of the nonzeros in $\bZ\trans\bZ$. There must (potentially)
be a nonzero in $\bL$ anywhere there is a nonzero in the lower
triangle of $\bZ\trans\bZ$ but other nonzeros in $\bL$ can be induced
during the course of the decomposition.  This is called ``fill-in''.
The extent of the fill-in can be changed by permuting the rows (and,
correspondingly, the columns) of $\bZ\trans\bZ$.  Because this
corresponds to a permutation of the columns of $\bZ$ we will refer to
such a permutation as a column permutation, with the understanding
that it causes a permutation of both the rows and columns of
$\bZ\trans\bZ$. 

General techniques for choosing a fill-reducing permutation, such as
graph-based techniques~\citep{Metis} or approximate minimal
degree~\citep{Davis:1996}, search for an arbitrary permutation.  For our
purposes there is a substantial advantage in choosing a fill-reducing
permutation that preserves separation of columns associated with
different grouping factors.  In particular, we can determine suitable
permutations based only on the grouping factors and their pairwise
crosstabulation.  That is, all we need to do is permute the levels of
each of the grouping factors based upon the positions of the nonzeros
in the pairwise crosstabulation.

If the grouping factors are a nested sequence of factors there will be
no fill-in.  In fact, both $\bL$ and its inverse will have exactly the
same pattern of nonzeros as does the lower triangle of $\bZ\trans\bZ$.
We do not seek a fill-reducing permutation if the grouping factors
form a nested sequence.  The case $k=1$ (the random effects are
determined by a single grouping factor) is, trivially, a nested
sequence.

Of the nontrivial cases we consider first two nonnested factors.

\subsection{Two non-nested grouping factors}
\label{sec:twofactor}

It is not unusual for the grouping factors to have very different
numbers of levels, as is the case in the Scottish secondary school
data.  Because the diagonal block for the first grouping factor is
preserved by the decomposition we put the factor with the largest
number of levels first.  In general we order the grouping factors
in decreasing order of the number of levels.

As shown in \citet{Liu:1990} and as implemented in the LDL package,
the number and positions of the off-diagonal elements in $\bL$ and in
$\bL^{-1}$ can be determined from the elimination tree of the matrix,
which depends only on the position of the first non-zero in each
column of the strict lower triangle of $\bZ\trans\bZ$.  Because
nonzeros in the strict lower triangle only occur in rows corresponding
to the second factor, there is no need to permute the levels of the
first factor.  (In the course of generating a permutation of the
levels of the second factor we will generate a permutation of the
levels of the first factor but that is only for the purposes of
illustration.)

Of the three outer blocks in $\bL$, we ignore the $(1,1)$ block
because it will be the identity and does not contribute any storage.
Because there are potentially $m_1 m_2$ elements in the $(2,1)$ block
and $m_1(m_1-1)/2$ elements in $(2,2)$ block (recall that the diagonal
elements are implicit), our heuristic is to concentrate on the fill-in
in the $(2,1)$ block and assume that the $(2,2)$ block will end up nearly
dense.  For the Scottish secondary school data we are concentrating on
the potential 2812 nonzeros in the $(2,1)$ block at the expense of the
171 potential nonzeros in the $(2,2)$ block.

Under this assumption a column in the $(2,1)$ block of $\bL^{-1}$ will
be filled from the first nonzero in $\bZ\trans\bZ$ down.  Therefore,
we permute the the levels of the second factor so as to concentrate
the nonzeros in the $(2,1)$ block on the lower rows.

First we will illustrate the results of the algorithm for determining
the permutation, then we will describe the algorithm in detail.
The permuted rows of $\bZ\trans\bZ$ for the Scottish secondary school
data the permuted rows are shown in Figure~\ref{fig:PermutedRows}.
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<FigPermutedRows,fig=TRUE,echo=FALSE,height=2.5,width=8.>>=
perm = .Call("sscCrosstab_groupedPerm", ctab)
trip = as(as(ctab, "sscMatrix"), "tripletMatrix")
trip@i = perm[trip@i + 1]
trip@j = perm[trip@j + 1]
print(image(trip,ylim=c(166.5,147.5),
            xlim=c(-0.5,166.5),colorkey=FALSE,aspect='xy'))
@   
  \caption{Locations of the non-zero elements in the last 19 rows of the
    $\bZ\trans\bZ$ matrix for model \var{fm1} fit to the
    \var{ScotsSec} data after permutation of the rows according to our
    algorithm.  The first 148 columns have been permuted to illustrate
    the algorithm.  This permutation does not affect the amount of
    fill-in in either $\bL$ or $\bL^{-1}$.
    Darker squares indicate larger magnitudes. Rows and columns are
    numbered from zero.}
  \label{fig:PermutedRows}
\end{figure}
The first 148 columns have been permuted to help illustrate the
algorithm.  This permutation does not affect the amount of fill-in in
either $\bL$ or $\bL^{-1}$.

The positions of the nonzero elements in the last 19 rows of $\bL$ are
shown in Figure~\ref{fig:Lnonzero} (the first 148 rows of $\bL$ are an
identity matrix augmented with 19 columns of zeros)
\begin{figure}[tbp]
  \centering
\setkeys{Gin}{width=\textwidth}
<<Figlnonzero,Fig=True,Echo=False,height=2.5,width=8>>=
print(image(as(fm1@rep, "tscMatrix"),ylim=c(166.5,147.5),
            xlim=c(-0.5,166.5),colorkey=FALSE,aspect='xy'))
@   
  \caption{Location of non-zero elements in the last 19 rows of the
    unit lower triangular matrix $\bL$ for model \var{fm1} fit to the
    \var{ScotsSec} data.  Darker squares indicate larger magnitudes.
    Rows and columns are numbered from zero.}
  \label{fig:Lnonzero}
\end{figure}

\subsection{Results on examples}
\label{sec:Examples}

\begin{table}[htbp]
  \centering
  \begin{tabular}[]{l r r r r r }
    \hline\\
    &&&Scottish&Dallas\\\hline
    levels of $\bff_1$ & 148 && 134713\\
    levels of $\bff_2$ & 19  && 3722\\
    diagonals          & 167 && 138435\\
    Sparse $\bZ\trans\bZ$       & 303 & 10.77 & 238676 & 0.05\\
    Dense storage               & 2812&& 501401786\\
    No\\
    $\bL$                       & 457 & 16.25 & 4707800 & 0.94\\
    $\bL^{-1}$                  & 1833 & 65.18& 404759242 & 80.73\\
    Metis\\
    $\bL$                       & 444 & 15.79 & 474827 & 0.09\\
    $\bL^{-1}$                  & 1855& 65.97 & 30144049 & 6.01\\
    Separated Metis\\
    $\bL$                       & 464 & 16.50 & 3763597 & 0.75\\
    $\bL^{-1}$                  & 2200&78.24& 266944262 &53.24\\
    Separated (greedy)\\
    $\bL$                       & 434 & 15.43 & 518642 & 0.10\\
    $\bL^{-1}$                  & 1544 & 54.91 & 17222447 & 3.43\\
    Separated (first)\\
    $\bL$                       & 432 & 15.36 & 519974 & 0.10\\
    $\bL^{-1}$                  & 1504 & 53.48 & 17238020 & 3.44\\
    Separated (last)\\
    $\bL$                       & 456 & 15.36 & 518576 & 0.10\\
    $\bL^{-1}$                  & 1574& 53.48 & 17222327 & 3.43\\
    \hline
  \end{tabular}
  \caption{Number of nonzero elements in $\bL$ and $\bL^{-1}$ for
    several different permutations applied to the Scottish secondary
    school data and the Dallas TAAS data.}
  \label{tab:Examples}
\end{table}
The small overhead incurred for index storage with sparse matrices is
repaid many times in the savings from not storing elements that are zero.

\section{Implementation}
\label{sec:Implementation}

As shown in the previous section, we store $\bZ\trans\bZ$ and
decompose $\bZ\trans\bZ+\bOmega=\bL\bD\bL\trans$ using sparse matrix
representations and the LDL package.  Many methods for
sparse matrices have both a symbolic phase, where the pattern of the
non-zero entries in the result is determined, and a numeric phase,
where the numerical results are determined.  In the LDL package these
are performed in different functions, which is a great advantage for
us because the symbolic phase need only be done once while the numeric
phase is repeated many times.

The number and positions of the non-zero elements in $\bZ\trans\bZ$,
$\bL$ and $\bL^{-1}$ can be determined from the pairwise
cross-tabulations of the grouping factors and the $q_i,i=1,\dots,k$.
We also determine the ordering of the levels within the groups, as
described in the next section, during the symbolic phase.  Once the
symbolic analysis is complete all the storage needed for intermediate
results can be allocated.  Only then are the model matrices $\bZ_i, i =
1,\dots,k$ and $\bX$ formed and expressions such as $\bZ\trans\bZ$ and
$\bZ\trans\bX$ evaluated.  Note that because the locations of the
non-zero elements in the products are known in advance, it is possible
to evaluate the $\bZ_i$ and $\bX$ row-wise (or in blocks of rows),
update the matrix products, and then discard the rows.  When $n$ is
very large and $p$ is moderate, sequential evaluation by rows could
result in considerably reduced memory usage.

As shown in \S\ref{sec:Generalizations} it is very convenient to allow
updating of the model matrices after the symbolic analysis because
iterative techniques for models such as the generalized linear mixed
model (GLMM) or the nonlinear mixed model (NLM) are often based on
successive linear mixed model solutions where the model matrices and
the ``working responses'' are updated between solutions.

In our implementation the vector $\bZ\trans\by$ is stored as the
$p+1$st column of $\bZ\trans\bX$ but we will write expressions as if
they were distinct.

Given a value of $\btheta$ we update $\bZ\trans\bZ$ to
$\bZ\trans\bZ+\bOmega$ and form its LDL decomposition
$\bZ\trans\bZ+\bOmega=\bL\bD\bL\trans$ (\emph{ldl\_numeric}).  The
diagonal matrix $\bD$ is stored as a vector of size $q$.  We form the
corresponding vector representation of $\bD^{-\frac{1}{2}}$.


\section{Permuting levels of factors}
\label{sec:Permuting}

