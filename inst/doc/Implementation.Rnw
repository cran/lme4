\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontshape=sl,
  fontfamily=courier,fontseries=b, fontsize=\scriptsize}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\vspace{-2.5ex}},fontfamily=courier,fontseries=b,%
  fontsize=\scriptsize}
%%\VignetteIndexEntry{Implementation Details}
%%\VignetteDepends{lme4}
\newcommand{\trans}{\ensuremath{^\mathsf{T}}}
\newcommand{\invtrans}{\ensuremath{^\mathsf{-T}}}
\title{Linear mixed model implementation in lme4}
\author{Douglas Bates\\Department of Statistics\\%
  University of Wisconsin -- Madison}
\begin{document}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,strip.white=TRUE}
\SweaveOpts{include=FALSE}
\setkeys{Gin}{width=\textwidth}
\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\package}[1]{\textsf{\small{#1}}}
\maketitle
\begin{abstract}
  We present the form of the model and details of the computational
  methods in \code{lmer} for linear mixed models and for generalized
  linear mixed models.  These techniques are illustrated on several
  examples.
\end{abstract}

<<preliminaries,echo=FALSE,print=FALSE>>=
library(lattice)
library(Matrix)
library(lme4)
data("Rail", package = "nlme")
Rail <- data.frame(travel = Rail$travel, Rail = Rail$Rail)
options(width=80, show.signif.stars = FALSE,
        lattice.theme = function() canonical.theme("pdf", color = FALSE))
@

\section{A simple example}
\label{sec:example}

The \code{Rail} data set from the \package{nlme} package is described
in \citet{pinheiro:bates:2000} as consisting of three measurements of
the travel time of a type of sound wave on each of six sample railroad
rails. We can examine the structure of these data with the \code{str}
function
<<strRail>>=
str(Rail)
@

Because there are only three observations on each of the rails a
dotplot (Figure~\ref{fig:Raildotplot}) shows the structure of the data
well.
<<Raildotplot,fig=TRUE,include=FALSE,width=8,height=4>>=
print(dotplot(Rail~travel,Rail,xlab="Travel time (ms)",ylab="Rail number"))
@ 
\begin{figure}[tb]
  \centering
  \includegraphics{Implementation-Raildotplot}
  \caption{Travel time of sound waves in a sample of six railroad
    rails.  There were three measurements of the travel time on each
    rail. The numbering of the rails has been reordered according to
    increasing mean travel time.}
  \label{fig:Raildotplot}
\end{figure}

In building a model for these data
<<Raildata>>=
Rail
@ 
we wish to characterize a typical travel time, say $\mu$, for the
population of such railroad rails and the deviations, say
$b_i,i=1,\dots,6$ of the individual rails from this population mean.
Because these specific rails are not of interest by themselves as much
as the variation in the population we model the $b_i$, which are
called the ``random effects'' for the rails, as having a normal
(Gaussian) distribution of the form $\mathcal{N}(0,\sigma^2_b)$.  The
$j$th measurement on the $i$th rail is expressed as
\begin{equation}
  \label{eq:1}
  y_{ij}=\mu+b_i+\epsilon_{ij}\quad
  b_i\sim\mathcal{N}(0,\sigma^2_b),\epsilon_{ij}\sim\mathcal{N}(0,\sigma^2)
  \quad  i=1,\dots,6\;j=1,\dots,3
\end{equation}

The parameters of this model are $\mu$, $\sigma^2_b$ and $\sigma^2$.
Technically the $b_i,i=1,\dots,6$ are not parameters but instead are
considered to be unobserved random variables for which we form
``predictions'' instead of ``estimates''.

To express generalizations of models like (\ref{eq:1}) more
conveniently we switch to a matrix/vector
representation in which the 18 observations of the travel time form
the response vector $\bm{y}$, the fixed-effect parameter $\mu$ forms a
$1$-dimensional column vector $\bm{\beta}$ and the six random effects
$b_i,i=1,\dots,6$ form the random effects vector $\bm{b}$.  The
structure of the data and the values of any covariates (none are used
in this model) are used to create model matrices $\bm{X}$ and
$\bm{Z}$.

Using these vectors and matrices and the $18$-dimensional vector
$\bm{\epsilon}$ that represents the per-observation noise terms the
model becomes
\begin{equation}
  \label{eq:lmm}
  \bm{y} = \bm{X}\bm{\beta} + \bm{Z}\bm{b}+\bm{\epsilon},\quad
  \bm{\epsilon}\sim\mathcal{N},\left(\bm{0},\sigma^2\bm{I}\right),\;
  \bm{b}\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{\Sigma}\right)\;\mathrm{and}\;
  \bm{b}\perp\bm{\epsilon}
\end{equation}

In the general form we write $p$ for the dimension of $\bm{\beta}$,
the fixed-effects parameter vector, and $q$ for the dimension of
$\bm{b}$, the vector of random effects.  Thus the model matrix
$\bm{X}$ has dimension $n\times p$, the model matrix $\bm{Z}$ has
dimension $n\times q$ and the relative variance-covariance matrix,
$\bm{\Sigma}$, for the random-effects has dimension $q\times q$.  The
symbol $\perp$ indicates independence of random variables and
$\mathcal{N}$ denotes the multivariate normal (Gaussian) distribution.

We say that matrix $\bm{\Sigma}$ is the relative variance-covariance
matrix of the random effects in the sense that it is the variance of
$\bm{b}$ relative to $\sigma^2$, the scalar variance of the
per-observation noise term $\bm{\epsilon}$.  Although it size, $q$,
can be very large, $\bm{\Sigma}$ is highly structured.  It is
symmetric, positive semi-definite and zero except for the diagonal
elements and certain elements close to the diagonal.

\subsection{Fitting the model and examining the results}
\label{sec:fitt-model-exam}

The maximum likelihood estimates for parameters in model (\ref{eq:1})
fit to the \code{Rail} data are obtained as
<<RailfitML>>=
Rm1ML <- lmer2(travel ~ 1 + (1|Rail), Rail, method = "ML",
               control = list(msVerbose = 1))
@ 
In this fit we have set the control parameter \code{msVerbose} to 1
indicating that information on the progress of the iterations should
be printed after every iteration.  Each line gives the iteration
number, the value of the deviance (negative twice the log-likelihood)
and the value of the parameter $s$ which is the standard deviation of
the random effects relative to the standard deviation of the
residuals. 

The printed form of the model
<<Railfitshow>>=
Rm1ML
@ 
provides additional information about the parameter estimates and some
of the measures of the fit such as the log-likelihood (-64.28), the
deviance for the maximum likelihood criterion (128.6), the deviance
for the REML criterion (122.2), Akaike's Information Criterion
(AIC$=132.6$) and Schwartz's Bayesian Information Criterion
(BIC$=134.3$).

The model matrices $\bm{Z}$ and $\bm{X}$ and the negative of the
response vector $-\bm{y}$ are stored in the \code{ZXyt} slot in the
transposed form.  Extracting the transpose of this slot
<<ZXytRail>>=
t(Rm1ML@ZXyt)
@ 
The first 6 columns of this matrix are $\bm{Z}$, the seventh column is
$\bm{X}$ and the eighth and final column is $-\bm{y}$.  As indicated
in the display of the matrix, it is stored as a sparse matrix.  The
elements represented as `.' are known to be zero and are not stored
explicitly.

The columns of $\bm{Z}$ are indicator columns (that is, the $i$th column has
a 1 in row $j$ if the $j$th observation is on rail $i$, otherwise it
is zero) but they are not in the usual ordering.  This is because the
levels of the \code{Rail} factor have been reordered according to
increasing mean response for Figure~\ref{fig:Raildotplot}.

The crossproduct of the columns of this matrix are stored as a
symmetric, sparse matrix in the \code{A} slot.
<<ARm1ML>>=
Rm1ML@A
@ 

The \code{L} component of this fitted model is a Cholesky
factorization of a matrix $\bm{A}^*(\bm{\theta})$ where $\bm{\theta}$ is a
parameter vector determining $\bm{\Sigma}(\bm{\theta})$.  This matrix can be
factored as $\bm{\Sigma}=\bm{T}\bm{S}\bm{S}\bm{T}\trans$, where
$\bm{T}$ is a unit, lower triangular matrix (that is, all the elements
above the diagonal are zero and all the elements on the diagonal are
unity) and $\bm{S}$ is a diagonal matrix with non-negative elements on
the diagonal.
The matrix $\bm{A}^*(\bm{\theta})$ is
\begin{equation}
  \label{eq:Astar}
  \begin{split}
    \bm{A}^*(\bm{\theta})&=
    \begin{bmatrix}
      {\bm{Z}^*}\trans\bm{Z}^*+\bm{I} & {\bm{Z}^*}\trans\bm{X} &
      -{\bm{Z}^*}\trans\bm{y}\\
      {\bm{X}}\trans\bm{Z}^* & {\bm{X}}\trans\bm{X} &
      -{\bm{X}}\trans\bm{y}\\
      -{\bm{y}}\trans\bm{Z}^*&-{\bm{y}}\trans\bm{X}&{\bm{y}}\trans\bm{y}
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      \bm{T}\trans\bm{S} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{I} & \bm{0} \\
      \bm{0} & \bm{0} & 1
    \end{bmatrix}
    \bm{A}
    \begin{bmatrix}
      \bm{S}\bm{T} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{I} & \bm{0} \\
      \bm{0} & \bm{0} & 1
    \end{bmatrix}+
    \begin{bmatrix}
      \bm{I} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{0} & \bm{0} \\
      \bm{0} & \bm{0} & 0
    \end{bmatrix} .
  \end{split}
\end{equation}

For model (\ref{eq:1}) the matrices $\bm{T}$ and $\bm{S}$ are
particularly simple, $\bm{T}=\bm{I}_6$, the $6\times 6$ identity matrix
and $\bm{S}=s_{1,1}\bm{I}_6$ where $s_{1,1}=\sigma_b/\sigma$ is the
standard deviation of the random effects relative to the standard
deviation of the per-observation noise term $\bm{\bm{\epsilon}}$.

<<fewdigits,echo=FALSE,results=hide>>=
op <- options(digits=4)
@ 

The Cholesky decomposition of $\bm{A}^*$ is a lower triangular sparse
matrix $\bm{L}$ 
<<LRm1ML>>=
as(Rm1ML@L, "sparseMatrix")
@ 
<<unfewdigits,echo=FALSE,results=hide>>=
options(op)
@

As explained in later sections the matrix $\bm{L}$ provides all the
information needed to evaluate the ML deviance or the REML deviance as
a function of $\bm{\theta}$.  The components of the deviance are given
in the \code{deviance} slot of the fitted model
<<devRm1ML>>=
Rm1ML@deviance
@
The element labelled \code{ldZ} is the logarithm of the square of
the determinant of the upper left $6\times 6$ section of $\bm{L}$.
This corresponds to
$\log\left|{\bm{Z}^*}\trans\bm{Z}^*+\bm{I}_q\right|$ where
$\bm{Z}^*=\bm{Z}\bm{T}\bm{S}$.  We can verify that the value
$27.38292$ can indeed be calculated in this way.
<<ldZRm1ML>>=
L <- as(Rm1ML@L, "sparseMatrix")
2*sum(log(diag(L)[1:6]))
@ 

The \code{lr2} element of the \code{deviance} slot is the logarithm of
the penalized residual sum of squares.  It can be calculated as the
logarithm of the square of the last diagonal element in $\bm{L}$.
<<lr2Rm1ML>>=
2*log(L[8,8])
@ 

For completeness we mention that the \code{ldX} element of the
\code{deviance} slot is the logarithm of the product of the squares of
the diagonal elements of $\bm{L}$ corresponding to columns of $\bm{X}$.
<<ldXRm1ML>>=
2*log(L[7,7])
@ 
This element is used in the calculation of the REML criterion.

Another slot in the fitted model object is \code{dims}, which 
contains information on the dimensions of the model and some of the
characteristics of the fit.
<<dimsRm1ML>>=
Rm1ML@dims
@
We can reconstruct the ML estimate of the residual variance as the
penalized residual sum of squares divided by the number of
observations.
%% FIXME: This is not calculated correctly in the current version of lmer2
<<s2Rm1ML>>=
exp(Rm1ML@deviance["lr2"])/Rm1ML@dims["n"]
@ 

The \emph{profiled deviance} function
\begin{equation}
  \label{eq:2}
  \begin{aligned}
  \tilde{\mathcal{D}}(\bm{\theta}) &= \log\left|{\bm{Z}^*}\trans\bm{Z}^*+\bm{I}_q\right| + 
  n \log \left(1+\frac{2\pi r^2}{n}\right) \\
  &= n\left[1+\log\left(\frac{2\pi}{n}\right)\right] +
    \log\left|{\bm{Z}^*}\trans\bm{Z}^*+\bm{I}_q\right| + n\log r^2
  \end{aligned}
\end{equation}
is a function of $\bm{\theta}$ only.  In this case
$\bm{\theta}=\sigma_1$, the relative standard deviation of the random
effects, is one-dimensional. The maximum likelihood estimate (mle) of
$\bm{\theta}$ minimizes the profiled deviance. The mle's of all the
other parameters in the model can be derived from the estimate of this
parameters.

<<devcomp,echo=FALSE,fig=TRUE,height=3.5>>=
mm <- Rm1ML
sg <- seq(0, 20, len = 101)
dev <- mm@deviance
nc <- length(dev)
nms <- names(dev)
vals <- matrix(0, nrow = length(sg), ncol = nc, dimnames = list(NULL, nms))
for (i in seq(along = sg)) {
    .Call("mer2_setPars", mm, sg[i], PACKAGE = "lme4")
    vals[i,] <- mm@deviance
}
vals[,"lr2"] <- mm@dims["n"] * vals[,"lr2"]
df <- data.frame(x = rep(sg, nc), y = as.vector(vals),
                 type = gl(nc, length(sg), labels = nms))
print(xyplot(y ~ x | type, df, type = c("g", "l"),
             subset = !(type %in% c("REML", "ldX")),
             scales = list(y = list(relation = "free")),
             layout = c(1,3)))
@ 

The term $n\left[1+\log\left(2\pi/n\right)\right]$ in (\ref{eq:2})
does not depend on $\bm{\theta}$.  The other two terms,
$\log\left|{\bm{Z}^*}\trans\bm{Z}^*+\bm{I}_q\right|$ and $n\log r^2$,
measure the complexity of the model and the fidelity of the fitted
values to the observed data, respectively.  We plot the value of each
of the varying terms versus $\sigma_1$ in Figure~\ref{fig:devcomp}.
\begin{figure}[tb]
  \centering
  \includegraphics{Implementation-devcomp}
  \caption{The deviance and those components that vary with
    $\bm{\theta}$ as a function of $\bm{\theta}$ (which is the
    scalar $\sigma_1$ in this case).}
  \label{fig:devcomp}
\end{figure}

The component $\log\left|\bm{S}\bm{Z}\trans\bm{Z}\bm{S}+\bm{I}\right|$
has the value $0$ at $\sigma_1=0$ and increases as $\sigma_1$
increases.  It is unbounded as $\sigma_1\rightarrow\infty$.  The
component $n\log\left(r^2\right)$ has a finite value at $\sigma_1=1$
from which it decreases as $\sigma_1$ increases.  The value at
$\sigma_1=0$ corresponds to the residual sum of squares for the
regression of $\bm{y}$ on the columns of $\bm{X}$.
<<leftlr2>>=
18 * log(deviance(lm(travel ~ 1, Rail)))
@ 
As $\sigma_1\rightarrow\infty$, $n\log\left(r^2\right)$ approaches the
value corresponding to the residual sum of squares for the regression
of $\bm{y}$ on the columns of $\bm{X}$ and $\bm{Z}$.  For this model
that is
<<rightlr2>>=
18 * log(deviance(lm(travel ~ Rail, Rail)))
@ 

<<devcomp2,echo=FALSE,fig=TRUE,height=4>>=
mm <- Rm1ML
sg <- seq(3.75, 8.25, len = 101)
dev <- mm@deviance
nc <- length(dev)
nms <- names(dev)
vals <- matrix(0, nrow = length(sg), ncol = nc, dimnames = list(NULL, nms))
for (i in seq(along = sg)) {
    .Call("mer2_setPars", mm, sg[i], PACKAGE = "lme4")
    vals[i,] <- mm@deviance
}
base <- 18 * log(deviance(lm(travel ~ Rail, Rail)))
print(xyplot(devC + ldZ ~ sg,
             data.frame(ldZ = vals[,"ldZ"], devC =
                        vals[, "ldZ"] + mm@dims['n'] * vals[, "lr2"] - base,
                        sg = sg), type = c("g", "l"),
             scales = list(x = list(axs = 'i')),
             xlab = expression(sigma[1]), ylab = "Shifted deviance",
             auto.key = list(text = c("deviance", "log|SZ'ZS + I|"),
             space = "right", points = FALSE, lines = TRUE)))
@ 
\begin{figure}[tb]
  \centering
  \includegraphics{Implementation-devcomp2}
  \caption{The part of the deviance that varies with $\sigma_1$ as a
    function of $\sigma_1$ near the optimum.  The component
    $\log\left|\bm{S}\bm{Z}\trans\bm{Z}\bm{S}+\bm{I}\right|$ is shown
    at the bottom of the frame.  This is the component of the deviance
    that increases with $\sigma_1$.  Added to this component is
    $n\log\left[r^2(\sigma_1)\right] - n\log\left[r^2(\infty)\right]$,
    the comonent of the deviance that decreases as $\sigma_1$
    increases.  Their sum is minimized at $\widehat{\sigma}_1=5.626$.}
  \label{fig:devcomp2}
\end{figure}

\section{Structure of $\bm{\Sigma}$ and $\bm{Z}$}
\label{sec:role-group-fact}

The columns of $\bm{Z}$ and the rows and columns of $\bm{\Sigma}$ are
associated with the levels of one or more grouping factors in the
data.  For example, a common application of linear mixed models is the
analysis of students' scores on the annual state-wide performance
tests mandated by the No Child Left Behind Act.  A given score is
associated with a student, a teacher, a school and a school district.
These could all be grouping factors in a model.

We write the grouping factors as $\bm{f}_i,i=1,\dots k$.  The number
of levels of the $i$th factor, $\bm{f}_i$, is $n_i$ and the number of
random effects associated with each level is $q_i$.  For example, if
$\bm{f}_1$ is ``student'' then $n_1$ is the number of students in the
study.  If we have a simple additive random effect for each student
then $q_1=1$.  If we have a random effect for both the intercept and
the slope with respect to time for each student then $q_1=2$.  The
$q_i,i=1,\dots,k$ are typically very small whereas the
$n_i,i=1,\dots,k$ can be very large.

In the statistical model we assume that random effects associated with
different grouping factors are independent, which implies that
$\bm{\Sigma}$ is block diagonal with $k$ diagonal blocks of sizes $n_i
q_i\times n_i q_i, i = 1,\dots,k$.  That is
\begin{equation}
  \label{eq:blockDiag}
  \bm{\Sigma}=
  \begin{bmatrix}
    \bm{\Sigma}_1 & \bm{0} & \hdots & \bm{0}\\
    \bm{0} & \bm{\Sigma}_2 & \hdots & \bm{0}\\
    \vdots & \vdots        & \ddots & \vdots \\
    \bm{0} & \bm{0}        & \hdots & \bm{\Sigma}_k
  \end{bmatrix}
\end{equation}
Furthermore, random effects associated
with different levels of the same grouping factor are assumed to be
independent and identically distributed, which implies that
$\bm{\Sigma}_i$ is itself block diagonal in $n_i$ blocks and that each
of these blocks is a copy of a $q_i\times q_i$ matrix
$\tilde{\bm{\Sigma}}_i$.  That is
\begin{equation}
  \label{eq:blockblock}
  \bm{\Sigma}_i=
    \begin{bmatrix}
    \tilde{\bm{\Sigma}}_i & \bm{0}     & \hdots & \bm{0}\\
    \bm{0}     & \tilde{\bm{\Sigma}}_i  & \hdots & \bm{0}\\
    \vdots    & \vdots    & \ddots & \vdots\\
    \bm{0}     & \bm{0}     & \hdots & \tilde{\bm{\Sigma}}_i
  \end{bmatrix}=
  \bm{I}_{n_i}\otimes\tilde{\bm{\Sigma}}_i\quad i=1,\dots,k
\end{equation}
where $\otimes$ denotes the Kronecker product.

The condition that $\bm{\Sigma}$ is positive semi-definite holds if
and only if the $\tilde{\bm{\Sigma}}_i,i=1,\dots,k$  are positive
semi-definite.  To ensure that the $\tilde{\bm{\Sigma}}_i$ are
positive semi-definite, we express them as
\begin{equation}
  \label{eq:STinner}
  \tilde{\bm{\Sigma}}_i=
  \tilde{\bm{T}}_i\tilde{\bm{S}}_i\tilde{\bm{S}}_i\tilde{\bm{T}}_i\trans,\quad
  i=1,\dots,k
\end{equation}
where $\tilde{\bm{T}}_i$ is a $q_i\times q_i$ unit lower-triangular
matrix (i.e. all the elements above the diagonal are zero and all the
diagonal elements are unity) and $\tilde{\bm{S}}_i$ is a $q_i\times
q_i$ diagonal matrix with non-negative elements on the diagonal. 

This is the ``LDL'' form of the Cholesky decomposition of positive
semi-definite matrices except that we express the diagonal matrix
$\bm{D}$, which is on the variance scale, as the square of the
diagonal matrix $\bm{S}$, which is on the standard deviation scale.
The profiled deviance behaves more like a quadratic on the standard
deviation scale than it does on the variance scale so the use of the
standard deviation scale enhances convergence.

The $n_i q_i\times n_i q_i$ matrices $\bm{S}_i,\bm{T}_i,\,i=1,\dots,k$
and the $q\times q$ matrices $\bm{S}$ and $\bm{T}$ are defined
analogously to (\ref{eq:blockblock}) and (\ref{eq:blockDiag}).  In
particular,
\begin{align}
  \bm{S}_i&=\bm{I}_{n_i}\otimes\tilde{\bm{S}}_i,\quad i=1,\dots,k \\
  \bm{T}_i&=\bm{I}_{n_i}\otimes\tilde{\bm{T}}_i,\quad i=1,\dots,k
\end{align}

Note that when $q_i=1$, $\tilde{\bm{T}}_i=\bm{I}$ and hence
$\bm{T}_i=\bm{I}$.  Furthermore, $\bm{S}_i$ is a multiple of the
identity matrix in this case.

The parameter vector $\bm{\theta}_i,i=1,\dots,k$ consists of the $q_i$
diagonal elements of $\tilde{\bm{S}}_i$, which are constrained to be
non-negative, followed by the $q_i(q_i-1)/2$ elements in the strict
lower triangle of $\tilde{\bm{T}}_i$ (in column-major ordering).
These last $q_i(q_i-1)/2$ elements are unconstrained.  The
$\bm{\theta}_i$ are combined as
\begin{displaymath}
  \bm{\theta}=
  \begin{bmatrix}
    \bm{\theta}_1 \\ \bm{\theta}_2 \\ \vdots \\ \bm{\theta}_k
  \end{bmatrix} .
\end{displaymath}
Each of the $q\times q$ matrices $\bm{S}$, $\bm{T}$ and $\bm{\Sigma}$
in the decomposition $\bm{\Sigma}=\bm{T}\bm{S}\bm{S}\bm{T}\trans$ is a
function of $\bm{\theta}$.  

As a unit triangular matrix $\bm{T}$ is non-singular.  That is,
$\bm{T}^{-1}$ exists and is easily calculated from the
$\tilde{\bm{T}}_i^{-1},i=1,\dots,k$.  When $\bm{\theta}$ is not on the
boundary defined by the constraints, $\bm{S}$ is a
diagonal matrix with strictly positive elements on the diagonal,
which implies that $\bm{S}^{-1}$ exists and that
$\bm{\Sigma}$ is non-singular with
$\bm{\Sigma}^{-1}=\bm{T}\invtrans\bm{S}^{-1}\bm{S}^{-1}\bm{T}^{-1}$.

When $\bm{\theta}$ is on the boundary the matrices $\bm{S}$
and $\bm{\Sigma}$ exist but are not invertible.  We say that
$\bm{\Sigma}$ is a \emph{degenerate} variance-covariance matrix in the
sense that one or more linear combinations of the vector $\bm{b}$ are
defined to have zero variance.  That is, the distribution of these
linear combinations is a point mass at 0.

The maximum likelihood estimates of $\bm{\theta}$ (or the restricted
maximum likelihood estimates, defined below) can be located on the
boundary.  That is, they can correspond to a degenerate
variance-covariance matrix and we must be careful to allow for this
case.  However, to begin we consider the non-degenerate case.

\section{Methods for non-singular $\bm{\Sigma}$}
\label{sec:non-singular}

When $\bm{\theta}$ is not on the boundary we can define a
standardized random effects vector 
\begin{equation}
  \label{eq:bstar}
  \bm{b}^*=\bm{S}^{-1}\bm{T}^{-1}\bm{b}
\end{equation}
with the properties
\begin{align}
  \mathsf{E}[\bm{b}^*]&=\bm{S}^{-1}\bm{T}^{-1}\mathsf{E}[\bm{b}]\\
  \mathsf{Var}[\bm{b}^*]&
  \begin{aligned}[t]
    &=\mathsf{E}[\bm{b}^*{\bm{b}^*}\trans]\\
    &=\bm{S}^{-1}\bm{T}^{-1}\mathsf{Var}[\bm{b}]\bm{T}\invtrans\bm{S}^{-1}\\
    &=\sigma^2\bm{S}^{-1}\bm{T}^{-1}\bm{\Sigma}\bm{T}\invtrans\bm{S}^{-1}\\
    &=\sigma^2\bm{S}^{-1}\bm{T}^{-1}\bm{T}\bm{S}\bm{S}\bm{T}\trans\bm{T}\invtrans\bm{S}^{-1}\\
    &=\sigma^2\bm{I}.
  \end{aligned}
\end{align}
Thus, the unconditional distribution of the $q$ elements of
$\bm{b}^*$ is
$\bm{b}^*\sim\mathcal{N}\left(\bm{0},\sigma^2\bm{I}\right)$, like that
of the $n$ elements of $\bm{\epsilon}$.

Obviously the transformation from $\bm{b}^*$ to $\bm{b}$ is 
\begin{equation}
  \label{eq:bstarb}
  \bm{b}=\bm{T}\bm{S}\bm{b}^*
\end{equation}
and the $n\times q$ model matrix for $\bm{b}^*$ is 
\begin{equation}
  \label{eq:Zstar}
  \bm{Z}^*=\bm{Z}\bm{T}\bm{S}
\end{equation}
so that
\begin{equation}
  \label{eq:Zb}
  \bm{Z}^*\bm{b}^*=\bm{Z}\bm{T}\bm{S}\bm{S}^{-1}\bm{T}^{-1}\bm{b}=\bm{Z}\bm{b} .
\end{equation}

Notice that $\bm{Z}^*$ can be evaluated even when
$\bm{\theta}$ is on the boundary.  Also, if we have a value of
$\bm{b}^*$ in such a case, we can evaluated $\bm{b}$ from $\bm{b}^*$.
 
Given the data $\bm{y}$ and values of $\bm{\theta}$ and $\bm{\beta}$,
the mode of the conditional distribution of $\bm{b}^*$ is the solution
to a penalized least squares problem
\begin{equation}
  \label{eq:bstarmode}
  \begin{split}
    \tilde{\bm{b}}^*(\bm{\theta},\bm{\beta}|\bm{y})&=
    \arg\min_{\bm{b}^*} \left[
    \left\|\bm{y}-\bm{X}\bm{\beta}-\bm{Z}^*\bm{b}^*\right\|^2
    +{\bm{b}^*}\trans\bm{b}^*\right]\\
    &=
    \arg\min_{\bm{b}^*}
    \left\|
      \begin{bmatrix}
        \bm{y}\\
        \bm{0}
      \end{bmatrix} -
      \begin{bmatrix}
        \bm{Z}^* & \bm{X}\\
        \bm{I}   & \bm{0}
      \end{bmatrix}
      \begin{bmatrix}
        \bm{b}^*\\
        \bm{\beta}
      \end{bmatrix}
      \right\|^2 .
  \end{split}
\end{equation}
In fact, if we optimize the penalized least squares expression in
(\ref{eq:bstarmode}) with respect to both $\bm{b}$ and $\bm{\beta}$ we
obtain the conditional estimates
$\widehat{\bm{\beta}}(\bm{\theta}|\bm{y})$ and the conditional modes 
$\tilde{\bm{b}}^*(\bm{\theta},\widehat{\bm{\beta}}(\bm{\theta})|\bm{y}))$
which we write as $\widehat{\bm{b}}^*(\bm{\theta})$.  That is,
\begin{equation}
  \label{eq:bstarbeta}
  \begin{split}
    \begin{bmatrix}
      \widehat{\bm{b}^*}(\bm{\theta})\\
      \widehat{\bm{\beta}}(\bm{\theta})
    \end{bmatrix} & =
    \arg\min_{\bm{b}^*,\bm{\beta}}
    \left\|
      \begin{bmatrix}
        \bm{Z}^* & \bm{X} & -\bm{y}\\
        \bm{I}   & \bm{0} &  \bm{0}
      \end{bmatrix}
      \begin{bmatrix}
        \bm{b}^*\\
        \bm{\beta}\\
        1
      \end{bmatrix}
    \right\|^2\\
     & =
    \arg\min_{\bm{b}^*,\bm{\beta}}
    \begin{bmatrix}
      \bm{b}^*\\
      \bm{\beta}\\
      1
    \end{bmatrix}\trans
    \bm{A}^*(\bm{\theta})
    \begin{bmatrix}
      \bm{b}^*\\
      \bm{\beta}\\
      1
    \end{bmatrix}
  \end{split}
\end{equation}
where the matrix $\bm{A}^*(\bm{\theta})$ is as shown in (\ref{eq:Astar})
and
\begin{equation}
  \label{eq:Amatrix}
  \bm{A}=
  \begin{bmatrix}
    \bm{Z}\trans\bm{Z} & \bm{Z}\trans\bm{X} & -\bm{Z}\trans\bm{y} \\
    \bm{X}\trans\bm{Z} & \bm{X}\trans\bm{X} & -\bm{X}\trans\bm{y} \\    
    -\bm{y}\trans\bm{Z} & -\bm{y}\trans\bm{X} & \bm{y}\trans\bm{y} \\    
  \end{bmatrix} .
\end{equation}
Note that $\bm{A}$ does not depend upon $\bm{\theta}$.  Furthermore,
the nature of the model matrices $\bm{Z}$ and $\bm{X}$ ensures that
the pattern of nonzeros in $\bm{A}^*(\bm{\theta})$ is the same as that
in $\bm{A}$.

Let the $q\times q$ permutation matrix $\bm{P}_Z$ represent a
fill-reducing permutation for $\bm{Z}\trans\bm{Z}$ and $\bm{P}_X$, of
size $p\times p$, represent a fill-reducing permutation for
$\bm{X}\trans\bm{X}$.  These could be determined, for example, using
the \emph{approximate minimal degree} (AMD) algorithm described in
\citet{davis06:csparse_book} and \citet{Davis:1996} and implemented in
both the \texttt{\small Csparse} \citep{Csparse} and the
\texttt{\small CHOLMOD} \citep{Cholmod} libraries of C functions.  (In
many cases $\bm{X}\trans\bm{X}$ is dense, but of small dimension
compared to $\bm{Z}\trans\bm{Z}$, and $\bm{Z}\trans\bm{X}$ is nearly
dense so $\bm{P}_X$ can be $\bm{I}_p$, the $p\times p$ identity
matrix.)

Let the permutation matrix $\bm{P}$ be
\begin{equation}
  \label{eq:fillPerm}
  \bm{P}=
  \begin{bmatrix}
    \bm{P}_Z & \bm{0} & \bm{0} \\
    \bm{0} & \bm{P}_X & \bm{0} \\
    \bm{0} & \bm{0} & 1
  \end{bmatrix} 
\end{equation}
and $\bm{L}(\bm{\theta})$ be the sparse Cholesky decomposition of
$\bm{A}^*(\bm{\theta})$ relative to this permutation.  That
is, $\bm{L}(\bm{\theta})$ is a sparse lower triangular matrix with the
property that
\begin{equation}
  \label{eq:Lmat}
  \bm{L}(\bm{\theta})\bm{L}(\bm{\theta})\trans=
  \bm{P}\bm{A}^*(\bm{\theta})\bm{P}\trans
\end{equation}

For $\bm{L}(\bm{\theta})$ to exist we must ensure that
$\bm{A}^*(\bm{\theta})$ is positive definite.  Examination of
(\ref{eq:bstarbeta}) shows that 
this will be true if
$\bm{X}$ is of full column rank and $\bm{y}$ does not lie in the
column span of $\bm{X}$ (or, in statistical terms, if we can't fit
$\bm{y}$ perfectly using only the fixed effects).

Let $r>0$ be the last element on the diagonal of $\bm{L}$.  Then the
minumum penalized residual sum of squares in (\ref{eq:bstarbeta}) is
$r^2$ and it occurs at $\widehat{\bm{b}^*}(\bm{\theta})$ and
$\hat{\beta}(\bm{\theta})$, the solutions to the sparse triangular system
\begin{equation}
  \label{eq:soln}
  \bm{L}(\bm{\theta})\trans\bm{P}
  \begin{bmatrix}
    \widehat{\bm{b}^*}(\bm{\theta})\\
    \widehat{\bm{\beta}}(\bm{\theta})\\
    1
  \end{bmatrix}=
  \begin{bmatrix}
    \bm{0}\\
    \bm{0}\\
    r
  \end{bmatrix}
\end{equation}
(Technically we should not write the $1$ in the solution; it should be
an unknown.  However, for $\bm{L}$ lower triangular with $r$ as the last
element on the diagonal and $\bm{P}$ a permutation that does not move
the last row, the solution for this ``unknown'' will always be $1$.)
Furthermore, $\log|{\bm{Z}^*}\trans\bm{Z}+\bm{I}|$ can be evaluated as
the sum of the logarithms of the first $q$ diagonal elements of
$\bm{L}(\bm{\theta})$.

The \emph{profiled deviance function},
$\tilde{\mathcal{D}}(\bm{\theta})$, which is negative twice the
log-likelihood of model (\ref{eq:lmm}) evaluated at
$\bm{\Sigma}(\bm{\theta})$, $\widehat{\bm{\beta}}(\bm{\theta})$ and
$\hat{\sigma}^2(\bm{\theta})$, can be expressed as
\begin{equation}
  \label{eq:dtheta}
  \tilde{\mathcal{D}}(\bm{\theta})=
  \log\left|{\bm{Z}^*}\trans\bm{Z}^*+\bm{I}\right|+
  n\left(1+\log\frac{2\pi r^2}{n}\right) .
\end{equation}

Notice that it is not necessary to solve for
$\widehat{\bm{\beta}}(\bm{\theta})$ or $\widehat{\bm{b}}^*(\bm{\theta})$
or $\widehat{\bm{b}}(\bm{\theta})$ to be able to evaluate
$d(\bm{\theta})$.  All that is needed is to update $\bm{A}$ to form
$\bm{A}^*$ from which the sparse Cholesky decomposition
$\bm{L}(\bm{\theta})$ can be calculated and
$\tilde{\mathcal{D}}(\bm{\theta})$ evaluated.

Once $\hat{\bm{\theta}}$ is determined we can solve for
$\widehat{\bm{\beta}}(\hat{\bm{\theta}})$
and $\widehat{\bm{b}}^*(\bm{\theta})$ using (\ref{eq:soln}) and for
\begin{equation}
  \label{eq:3}
  \widehat{\sigma}^2(\hat{\bm{\theta}})=\frac{r^2(\hat{\bm{\theta}})}{n} .
\end{equation}
Furthermore,
$\widehat{\bm{b}}(\hat{\bm{\theta}})=
\bm{S}\bm{T}\widehat{\bm{b}}^*(\hat{\bm{\theta}})$.

\section{Methods for singular $\bm{\Sigma}$}
\label{sec:singular}

When $\bm{\theta}$ is on the boundary, corresponding to a singular
$\bm{\Sigma}$, some of the columns of $\bm{Z}^*$ are zero.  However,
the matrix $\bm{A}^*$ is non-singular and elements of $\bm{b}^*$
corresponding to the zeroed columns in $\bm{Z}^*$ approach zero
smoothly as $\bm{\theta}$ approaches the boundary.  Thus
$r(\bm{\theta})$ and $\left|{\bm{Z}^*}\trans\bm{Z}+\bm{I}\right|$ are
well-defined, as are $\tilde{\mathcal{D}}(\bm{\theta})$ and the
conditional modes $\widehat{\bm{b}}(\bm{\theta})$.

In other words, (\ref{eq:Astar}) and (\ref{eq:Lmat}) can be used to
define $\tilde{\mathcal{D}}(\bm{\theta})$ whether or not $\bm{\theta}$
is on the boundary.

\section{REML estimates}
\label{sec:reml-estimates}

It is common to estimate the per-observation noise variance $\sigma^2$
in a fixed-effects linear model as $\hat{\sigma}^2=r^2/(n-p)$ where
$r^2$ is the (unpenalized) residual sum-of-squares, $n$ is the number
of observations and $p$ is the number of fixed-effects parameters.
This is not the maximum likelihood estimate of $\sigma^2$, which is
$r^2/n$.  It is the ``restricted'' or ``residual'' maximum likelihood
(REML) estimate, which takes into account that the residual vector
$\bm{y}-\hat{\bm{y}}$ is constrained to a linear subspace of dimension
$n-p$ in the response space.  Thus its squared length,
$\left\|bm{y}-\hat{\bm{y}}\right\|^2=r^2$, has only $n=p$
\emph{degrees of freedom} associated with it.

The profiled REML deviance for a linear mixed model can be expressed as
\begin{equation}
  \label{eq:REMLdev}
  \tilde{\mathcal{D}}_R(\bm{\theta})=
  \log\left|{\bm{Z}^*}\trans\bm{Z}^*+\bm{I}\right|+
  (n-p)\left(1+\log\frac{2\pi r^2}{n-p}\right) .
\end{equation}



\bibliography{lme4}
\end{document}
